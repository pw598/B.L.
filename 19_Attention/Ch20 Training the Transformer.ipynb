{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f982e682-2f30-49ae-b8fe-cf43cbdfebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64\n",
    "\n",
    "\n",
    "class PrepareDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = 10000  # Number of sentences to include in the dataset\n",
    "        self.train_split = 0.9  # Ratio of the training data split\n",
    "\n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "\n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "\n",
    "        # Include start and end of string tokens\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "        # Random shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "\n",
    "        # Split the dataset\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "\n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "\n",
    "        # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "\n",
    "        return (trainX, trainY, train, enc_seq_length, dec_seq_length,\n",
    "                enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94643f3-3481-48f5-ad0f-9c62448e8262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> i love astronomy <EOS> \n",
      " tf.Tensor([   1    3   49 1429    2    0    0], shape=(7,), dtype=int64)\n",
      "Encoder sequence length: 7\n",
      "<START> ich liebe die astronomie <EOS> \n",
      " tf.Tensor([   1    3   59   25 1876    2    0    0    0    0    0    0], shape=(12,), dtype=int64)\n",
      "Decoder sequence length: 12\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy.random import shuffle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64\n",
    "\n",
    "\n",
    "class PrepareDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = 10000  # Number of sentences to include in the dataset\n",
    "        self.train_split = 0.9  # Ratio of the training data split\n",
    "\n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "\n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "\n",
    "        # Include start and end of string tokens\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "        # Random shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "\n",
    "        # Split the dataset\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "\n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
    "\n",
    "        # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the input sequences\n",
    "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
    "\n",
    "        return (trainX, trainY, train, enc_seq_length, dec_seq_length,\n",
    "                enc_vocab_size, dec_vocab_size)\n",
    "\n",
    "\n",
    "# Prepare the training data\n",
    "dataset = PrepareDataset()\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, \\\n",
    "    enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "print(train_orig[0, 0], '\\n', trainX[0, :])\n",
    "print('Encoder sequence length:', enc_seq_length)\n",
    "print(train_orig[0, 1], '\\n', trainY[0, :])\n",
    "print('Decoder sequence length:', dec_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac45fd-e84c-4dc0-afe6-fb0c4c553151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, \\\n",
    "    float32, GradientTape, TensorSpec, function, int64\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from model import TransformerModel\n",
    "from prepare_dataset import PrepareDataset\n",
    "from time import time\n",
    "\n",
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Implementing a learning rate scheduler\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = cast(warmup_steps, float32)\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and\n",
    "        # decreasing it thereafter\n",
    "        step_num = cast(step_num, float32)\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n",
    "\n",
    "# Instantiate an Adam optimizer\n",
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "\n",
    "# Prepare the training and test splits of the dataset\n",
    "dataset = PrepareDataset()\n",
    "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, \\\n",
    "    enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "# Prepare the dataset batches\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,\n",
    "                                  dec_seq_length, h, d_k, d_v, d_model, d_ff, n,\n",
    "                                  dropout_rate)\n",
    "\n",
    "# Defining the loss function\n",
    "def loss_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of loss\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(mask)\n",
    "\n",
    "# Defining the accuracy function\n",
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of accuracy\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)\n",
    "\n",
    "# Include metrics monitoring\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)\n",
    "\n",
    "# Speeding up the training process\n",
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Compute the training loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)\n",
    "\n",
    "start_time = time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1} Step {step} Loss {train_loss.result():.4f} \"\n",
    "                  + f\"Accuracy {train_accuracy.result():.4f}\")\n",
    "\n",
    "    # Print epoch number and loss value at the end of every epoch\n",
    "    print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \"\n",
    "          + f\"Training Accuracy {train_accuracy.result():.4f}\")\n",
    "\n",
    "    # Save a checkpoint after every five epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
