{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd2c649-3c44-4dc7-ab2c-211889cbc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "                       **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n",
    "                                                          d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate)\n",
    "                              for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd221492-ce15-4874-8902-8f5b050da057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.44225287 -0.1418201   0.27835715 ... -0.28114012 -0.3306624\n",
      "    0.03308968]\n",
      "  [-1.1595763   0.25144437  0.9103069  ... -0.51401526  0.27655602\n",
      "    0.16603595]\n",
      "  [-0.5247609  -0.23343396  0.460029   ... -0.46442458  0.8576267\n",
      "    0.07448453]\n",
      "  [-0.35117087 -0.15650626  0.20488194 ...  0.20953347  0.4093092\n",
      "   -0.39760205]\n",
      "  [-0.7069898   0.10135477  0.69028014 ... -0.48624736 -0.07287742\n",
      "    0.5317908 ]]\n",
      "\n",
      " [[-1.2777957  -0.10262299  1.203131   ...  0.43745124  0.04969597\n",
      "   -0.5792841 ]\n",
      "  [-0.56312644  0.00727174  0.10930679 ... -0.25434405  0.16089956\n",
      "    0.8021291 ]\n",
      "  [-0.6501373  -0.6845092   0.10021717 ... -0.08072834 -0.12161421\n",
      "   -0.78274447]\n",
      "  [ 0.42991608 -1.1257325   0.14893529 ...  0.20001248  0.16570781\n",
      "    0.34833914]\n",
      "  [-0.7636283   0.28752598  0.1771816  ... -0.52849245 -0.1759928\n",
      "    0.18713282]]\n",
      "\n",
      " [[-0.64861476  0.47355896  0.77302766 ... -0.12366288 -0.33762988\n",
      "   -0.03607207]\n",
      "  [-0.5166224  -0.19021413 -0.07040275 ...  0.19798204  0.31171423\n",
      "   -0.23768672]\n",
      "  [-0.9721769  -0.17376077  0.11987749 ... -0.14289965  0.02800742\n",
      "   -0.08883408]\n",
      "  [-1.3295414  -0.19007784  0.4905068  ... -0.32174343  0.26288784\n",
      "    0.22868602]\n",
      "  [-1.1939803   0.44705233  0.54787534 ... -0.05106481  0.01772441\n",
      "   -0.08122757]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.7691235  -0.13248311  1.2076125  ... -0.54305255  0.08143935\n",
      "    0.9954404 ]\n",
      "  [-1.0885826   0.44019955  1.0396641  ... -0.3625514  -0.22218631\n",
      "    0.8061929 ]\n",
      "  [-0.9222859  -0.47009155  0.5220892  ... -0.76893556 -0.1198296\n",
      "   -0.08183946]\n",
      "  [-0.6268769  -0.41126993  1.3138006  ... -0.54094684 -0.6992926\n",
      "    0.49082145]\n",
      "  [-0.36551055 -0.17654651  0.02978318 ... -0.50923926 -0.14356177\n",
      "    0.7266608 ]]\n",
      "\n",
      " [[-0.5599111  -0.4480842   0.7086383  ... -0.01021165  0.3000989\n",
      "   -0.37824458]\n",
      "  [-0.27182963 -0.54382795  0.52372175 ...  0.27072877 -0.33220264\n",
      "    0.09757884]\n",
      "  [ 0.8329752  -0.54101825  0.32277983 ...  0.07236034 -0.12199333\n",
      "   -0.37785068]\n",
      "  [-0.42534572 -1.0755346   0.82954884 ...  0.21918142 -0.8065456\n",
      "   -0.25066137]\n",
      "  [-0.5043978  -1.4053837   0.60569406 ... -0.10975521  0.3748067\n",
      "   -0.07633641]]\n",
      "\n",
      " [[-0.77203715 -0.18592781 -0.641375   ...  0.5064443   0.75046605\n",
      "    0.49930003]\n",
      "  [-1.11485    -1.2874755  -0.32174775 ... -0.451623    0.88849044\n",
      "   -0.22908582]\n",
      "  [-0.9997187   0.84430665 -0.6081249  ... -0.35161695  0.93435293\n",
      "    0.17275192]\n",
      "  [-1.1104195  -0.31715015 -0.3381076  ... -0.3686604   0.97726923\n",
      "    0.7505065 ]\n",
      "  [-1.2955992  -0.6239539  -0.48569992 ... -0.05145286  0.4447708\n",
      "   -0.182202  ]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate,\n",
    "                       **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size,\n",
    "                                                          d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate)\n",
    "                              for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n,\n",
    "                  dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
