{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab533509-2f12-4b43-b1b3-2f02e3765d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.46470755 0.7384408  0.2860181  ... 0.31849805 0.48213288 0.4744081 ]\n",
      "  [0.45854688 0.7264023  0.2824839  ... 0.3174438  0.47815514 0.46740547]\n",
      "  [0.46358797 0.7459711  0.28433827 ... 0.32279712 0.48154926 0.49903095]\n",
      "  [0.47782344 0.747559   0.29384312 ... 0.32176462 0.48443544 0.46941072]\n",
      "  [0.46284714 0.7325383  0.26369342 ... 0.30441666 0.50560844 0.4716068 ]]\n",
      "\n",
      " [[0.60151136 0.27857032 0.62798256 ... 0.46278474 0.26041555 0.42701924]\n",
      "  [0.6013931  0.2806837  0.6344434  ... 0.4687489  0.26205397 0.42693996]\n",
      "  [0.60211295 0.26842213 0.6249392  ... 0.45708007 0.25561857 0.4305825 ]\n",
      "  [0.59268886 0.30392352 0.61443347 ... 0.4551766  0.27553713 0.44794706]\n",
      "  [0.6004778  0.2865997  0.6208826  ... 0.46500584 0.2733471  0.43604922]]\n",
      "\n",
      " [[0.59839356 0.65023565 0.4653538  ... 0.56444085 0.70241755 0.64458144]\n",
      "  [0.5837182  0.62851477 0.4857946  ... 0.550605   0.7079192  0.65832466]\n",
      "  [0.5659439  0.61847943 0.47208583 ... 0.54991466 0.7142468  0.67001987]\n",
      "  [0.60217845 0.629879   0.48679012 ... 0.56544113 0.7095448  0.6551027 ]\n",
      "  [0.5602871  0.60357237 0.4671761  ... 0.56839424 0.7185025  0.68153334]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.40216762 0.48347172 0.69494134 ... 0.43011433 0.32902235 0.38183066]\n",
      "  [0.39948243 0.47139728 0.6867274  ... 0.42424715 0.34220698 0.37959528]\n",
      "  [0.43378222 0.4672782  0.7012485  ... 0.44676596 0.332093   0.3663588 ]\n",
      "  [0.4238279  0.48707974 0.70531917 ... 0.45121184 0.31904596 0.37963802]\n",
      "  [0.4424467  0.47526848 0.7075031  ... 0.4578451  0.31695825 0.3691295 ]]\n",
      "\n",
      " [[0.41061568 0.42238513 0.72771984 ... 0.42964968 0.5334067  0.4478655 ]\n",
      "  [0.4146101  0.43447655 0.7255101  ... 0.4250535  0.5550356  0.4410227 ]\n",
      "  [0.4092878  0.42910868 0.7260181  ... 0.41156948 0.549623   0.44494858]\n",
      "  [0.40985757 0.43274194 0.7259332  ... 0.41599628 0.5462291  0.4512302 ]\n",
      "  [0.40981394 0.42803884 0.72348094 ... 0.44068563 0.53111684 0.44768563]]\n",
      "\n",
      " [[0.51415324 0.37099695 0.6275503  ... 0.4655941  0.5441143  0.6818885 ]\n",
      "  [0.50915545 0.384717   0.6359008  ... 0.49122185 0.51755357 0.66148305]\n",
      "  [0.5001471  0.3717666  0.6281916  ... 0.4873794  0.53009045 0.6825336 ]\n",
      "  [0.5103773  0.36895978 0.6272471  ... 0.48934639 0.52101076 0.6689014 ]\n",
      "  [0.5044635  0.38264322 0.6376124  ... 0.5015606  0.52466244 0.6759508 ]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "from tensorflow import matmul, math, cast, float32\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.backend import softmax\n",
    "\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "batch_size = 64  # Batch size from the training process\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
