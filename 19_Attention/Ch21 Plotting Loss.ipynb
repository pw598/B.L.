{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5038871b-b0ba-4982-aa80-50cb555cb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load, dump, HIGHEST_PROTOCOL\n",
    "from numpy.random import shuffle\n",
    "from numpy import savetxt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import convert_to_tensor, int64\n",
    "\n",
    "class PrepareDataset:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_sentences = 10000  # Number of sentences to include in the dataset\n",
    "        self.train_split = 0.8  # Ratio of the training data split\n",
    "        self.val_split = 0.1  # Ratio of the validation data split\n",
    "\n",
    "    # Fit a tokenizer\n",
    "    def create_tokenizer(self, dataset):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def find_seq_length(self, dataset):\n",
    "        return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "    def find_vocab_size(self, tokenizer, dataset):\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "        return len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Encode and pad the input sequences\n",
    "    def encode_pad(self, dataset, tokenizer, seq_length):\n",
    "        x = tokenizer.texts_to_sequences(dataset)\n",
    "        x = pad_sequences(x, maxlen=seq_length, padding='post')\n",
    "        x = convert_to_tensor(x, dtype=int64)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def save_tokenizer(self, tokenizer, name):\n",
    "        with open(name + '_tokenizer.pkl', 'wb') as handle:\n",
    "            dump(tokenizer, handle, protocol=HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __call__(self, filename, **kwargs):\n",
    "        # Load a clean dataset\n",
    "        clean_dataset = load(open(filename, 'rb'))\n",
    "\n",
    "        # Reduce dataset size\n",
    "        dataset = clean_dataset[:self.n_sentences, :]\n",
    "\n",
    "        # Include start and end of string tokens\n",
    "        for i in range(dataset[:, 0].size):\n",
    "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
    "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
    "\n",
    "        # Random shuffle the dataset\n",
    "        shuffle(dataset)\n",
    "\n",
    "        # Split the dataset in training, validation and test sets\n",
    "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
    "        val = dataset[int(self.n_sentences * self.train_split):\n",
    "                      int(self.n_sentences * (1-self.val_split))]\n",
    "        test = dataset[int(self.n_sentences * (1 - self.val_split)):]\n",
    "\n",
    "        # Prepare tokenizer for the encoder input\n",
    "        enc_tokenizer = self.create_tokenizer(dataset[:, 0])\n",
    "        enc_seq_length = self.find_seq_length(dataset[:, 0])\n",
    "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "        # Prepare tokenizer for the decoder input\n",
    "        dec_tokenizer = self.create_tokenizer(dataset[:, 1])\n",
    "        dec_seq_length = self.find_seq_length(dataset[:, 1])\n",
    "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "        # Encode and pad the training input\n",
    "        trainX = self.encode_pad(train[:, 0], enc_tokenizer, enc_seq_length)\n",
    "        trainY = self.encode_pad(train[:, 1], dec_tokenizer, dec_seq_length)\n",
    "\n",
    "        # Encode and pad the validation input\n",
    "        valX = self.encode_pad(val[:, 0], enc_tokenizer, enc_seq_length)\n",
    "        valY = self.encode_pad(val[:, 1], dec_tokenizer, dec_seq_length)\n",
    "\n",
    "        # Save the encoder tokenizer\n",
    "        self.save_tokenizer(enc_tokenizer, 'enc')\n",
    "\n",
    "        # Save the decoder tokenizer\n",
    "        self.save_tokenizer(dec_tokenizer, 'dec')\n",
    "\n",
    "        # Save the testing dataset into a text file\n",
    "        savetxt('test_dataset.txt', test, fmt='%s')\n",
    "\n",
    "        return (trainX, trainY, valX, valY, train, val, enc_seq_length,\n",
    "                dec_seq_length, enc_vocab_size, dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e15a28f-979e-43ba-ba51-a69f9acd75ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 12 2404 3864\n",
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 Step 0 Loss 8.3382 Accuracy 0.0000\n",
      "Epoch 1 Step 50 Loss 7.6370 Accuracy 0.1366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, \\\n",
    "    float32, GradientTape, function\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from model import TransformerModel\n",
    "from prepare_dataset import PrepareDataset\n",
    "from time import time\n",
    "from pickle import dump\n",
    "\n",
    "# Define the model parameters\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of model layers' outputs\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "# Define the training parameters\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 1e-9\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Implementing a learning rate scheduler\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = cast(warmup_steps, float32)\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and\n",
    "        # decreasing it thereafter\n",
    "        step_num = cast(step_num, float32)\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n",
    "\n",
    "# Instantiate an Adam optimizer\n",
    "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)\n",
    "\n",
    "# Prepare the training dataset\n",
    "dataset = PrepareDataset()\n",
    "trainX, trainY, valX, valY, train_orig, val_orig, enc_seq_length, \\\n",
    "    dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')\n",
    "\n",
    "print(enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size)\n",
    "\n",
    "# Prepare the training dataset batches\n",
    "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset batches\n",
    "val_dataset = data.Dataset.from_tensor_slices((valX, valY))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length,\n",
    "                                  dec_seq_length, h, d_k, d_v, d_model, d_ff, n,\n",
    "                                  dropout_rate)\n",
    "\n",
    "# Defining the loss function\n",
    "def loss_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of loss\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "    mask = cast(mask, float32)\n",
    "\n",
    "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
    "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * mask\n",
    "\n",
    "    # Compute the mean loss over the unmasked values\n",
    "    return reduce_sum(loss) / reduce_sum(mask)\n",
    "\n",
    "\n",
    "# Defining the accuracy function\n",
    "def accuracy_fcn(target, prediction):\n",
    "    # Create mask so that the zero padding values are not included in the\n",
    "    # computation of accuracy\n",
    "    mask = math.logical_not(equal(target, 0))\n",
    "\n",
    "    # Find equal prediction and target values, and apply the padding mask\n",
    "    accuracy = equal(target, argmax(prediction, axis=2))\n",
    "    accuracy = math.logical_and(mask, accuracy)\n",
    "\n",
    "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
    "    mask = cast(mask, float32)\n",
    "    accuracy = cast(accuracy, float32)\n",
    "\n",
    "    # Compute the mean accuracy over the unmasked values\n",
    "    return reduce_sum(accuracy) / reduce_sum(mask)\n",
    "\n",
    "# Include metrics monitoring\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = Mean(name='train_accuracy')\n",
    "val_loss = Mean(name='val_loss')\n",
    "\n",
    "# Create a checkpoint object and manager to manage multiple checkpoints\n",
    "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
    "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=None)\n",
    "\n",
    "# Initialise dictionaries to store the training and validation losses\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "# Speeding up the training process\n",
    "@function\n",
    "def train_step(encoder_input, decoder_input, decoder_output):\n",
    "    with GradientTape() as tape:\n",
    "\n",
    "        # Run the forward pass of the model to generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
    "\n",
    "        # Compute the training loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "\n",
    "        # Compute the training accuracy\n",
    "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
    "\n",
    "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
    "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
    "\n",
    "    # Update the values of the trainable variables by gradient descent\n",
    "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy)\n",
    "\n",
    "start_time = time()\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
    "\n",
    "    # Iterate over the dataset batches\n",
    "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = train_batchX[:, 1:]\n",
    "        decoder_input = train_batchY[:, :-1]\n",
    "        decoder_output = train_batchY[:, 1:]\n",
    "\n",
    "        train_step(encoder_input, decoder_input, decoder_output)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} \"\n",
    "                  + f\"Accuracy {train_accuracy.result():.4f}\")\n",
    "\n",
    "    # Run a validation step after every epoch of training\n",
    "    for val_batchX, val_batchY in val_dataset:\n",
    "        # Define the encoder and decoder inputs, and the decoder output\n",
    "        encoder_input = val_batchX[:, 1:]\n",
    "        decoder_input = val_batchY[:, :-1]\n",
    "        decoder_output = val_batchY[:, 1:]\n",
    "\n",
    "        # Generate a prediction\n",
    "        prediction = training_model(encoder_input, decoder_input, training=False)\n",
    "\n",
    "        # Compute the validation loss\n",
    "        loss = loss_fcn(decoder_output, prediction)\n",
    "        val_loss(loss)\n",
    "\n",
    "    # Print epoch number and accuracy and loss values at the end of every epoch\n",
    "    print(f\"Epoch {epoch+1}: Training Loss {train_loss.result():.4f}, \"\n",
    "          + f\"Training Accuracy {train_accuracy.result():.4f}, \"\n",
    "          + f\"Validation Loss {val_loss.result():.4f}\")\n",
    "\n",
    "    # Save a checkpoint after every epoch\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "        # Save the trained model weights\n",
    "        training_model.save_weights(\"weights/wghts\" + str(epoch + 1) + \".ckpt\")\n",
    "\n",
    "        train_loss_dict[epoch] = train_loss.result()\n",
    "        val_loss_dict[epoch] = val_loss.result()\n",
    "\n",
    "# Save the training loss values\n",
    "with open('./train_loss.pkl', 'wb') as file:\n",
    "    dump(train_loss_dict, file)\n",
    "\n",
    "# Save the validation loss values\n",
    "with open('./val_loss.pkl', 'wb') as file:\n",
    "    dump(val_loss_dict, file)\n",
    "\n",
    "print(\"Total time taken: %.2fs\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec8f98d-2f29-4fbe-8224-da793951b55e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from matplotlib.pylab import plt\n",
    "from numpy import arange\n",
    "\n",
    "# Load the training and validation loss dictionaries\n",
    "train_loss = load(open('train_loss.pkl', 'rb'))\n",
    "val_loss = load(open('val_loss.pkl', 'rb'))\n",
    "\n",
    "# Retrieve each dictionary's values\n",
    "train_values = train_loss.values()\n",
    "val_values = val_loss.values()\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, 21)\n",
    "\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_values, label='Training Loss')\n",
    "plt.plot(epochs, val_values, label='Validation Loss')\n",
    "\n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Set the tick locations\n",
    "plt.xticks(arange(0, 21, 2))\n",
    "\n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4c012-8387-4a6e-99c0-3f07a21596a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
