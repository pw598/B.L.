{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# define a small classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=3, random_state=1)\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.810 (0.032)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a decision tree on the entire small dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=3, n_informative=2, n_redundant=1, random_state=1)\n",
    "\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report result\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">f([0, 1, 2, 3, 4]) = 0.820000 \n",
      ">f([0, 1, 2, 3]) = 0.827333 \n",
      ">f([0, 1, 2, 4]) = 0.818667 \n",
      ">f([0, 1, 2]) = 0.819667 \n",
      ">f([0, 1, 3, 4]) = 0.821667 \n",
      ">f([0, 1, 3]) = 0.825667 \n",
      ">f([0, 1, 4]) = 0.808667 \n",
      ">f([0, 1]) = 0.815333 \n",
      ">f([0, 2, 3, 4]) = 0.824333 \n",
      ">f([0, 2, 3]) = 0.827333 \n",
      ">f([0, 2, 4]) = 0.826333 \n",
      ">f([0, 2]) = 0.818667 \n",
      ">f([0, 3, 4]) = 0.826667 \n",
      ">f([0, 3]) = 0.822000 \n",
      ">f([0, 4]) = 0.816000 \n",
      ">f([0]) = 0.639333 \n",
      ">f([1, 2, 3, 4]) = 0.822000 \n",
      ">f([1, 2, 3]) = 0.819000 \n",
      ">f([1, 2, 4]) = 0.822333 \n",
      ">f([1, 2]) = 0.821333 \n",
      ">f([1, 3, 4]) = 0.818667 \n",
      ">f([1, 3]) = 0.822000 \n",
      ">f([1, 4]) = 0.806667 \n",
      ">f([1]) = 0.797000 \n",
      ">f([2, 3, 4]) = 0.830667 \n",
      ">f([2, 3]) = 0.755333 \n",
      ">f([2, 4]) = 0.831667 \n",
      ">f([2]) = 0.516667 \n",
      ">f([3, 4]) = 0.826667 \n",
      ">f([3]) = 0.514333 \n",
      ">f([4]) = 0.777667 \n",
      "Done!\n",
      "f([2, 4]) = 0.831667\n"
     ]
    }
   ],
   "source": [
    "# feature selection by enumerating all possible subsets of features\n",
    "from itertools import product\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=3, random_state=1)\n",
    "\n",
    "# determine the number of columns\n",
    "n_cols = X.shape[1]\n",
    "\n",
    "best_subset, best_score = None, 0.0\n",
    "\n",
    "# enumerate all combinations of input features\n",
    "for subset in product([True, False], repeat=n_cols):\n",
    "    \n",
    "    # convert into column indexes\n",
    "    ix = [i for i, x in enumerate(subset) if x]\n",
    "    \n",
    "    # check for now column (all False)\n",
    "    if len(ix) == 0:\n",
    "        continue\n",
    "        \n",
    "    # select columns\n",
    "    X_new = X[:, ix]\n",
    "    \n",
    "    # define model\n",
    "    model = DecisionTreeClassifier()\n",
    "    \n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X_new, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # summarize scores\n",
    "    result = mean(scores)\n",
    "    \n",
    "    # report progress\n",
    "    print('>f(%s) = %f ' % (ix, result))\n",
    "    \n",
    "    # check if it is better than the best so far\n",
    "    if best_score is None or result >= best_score:\n",
    "        \n",
    "        # better result\n",
    "        best_subset, best_score = ix, result\n",
    "        \n",
    "# report best\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best_subset, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 500) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# define a large classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=500, n_informative=10, n_redundant=490, random_state=1)\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.916 (0.002)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a decision tree on the entire larger dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=500, n_informative=10, n_redundant=490, random_state=1)\n",
    "\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# define evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report result\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 f(260) = 0.907800\n",
      ">2 f(255) = 0.907800\n",
      ">3 f(265) = 0.907800\n",
      ">4 f(267) = 0.907800\n",
      ">5 f(262) = 0.907800\n",
      ">6 f(262) = 0.908299\n",
      ">7 f(264) = 0.908299\n",
      ">8 f(260) = 0.908299\n",
      ">9 f(256) = 0.908299\n",
      ">10 f(260) = 0.908399\n",
      ">11 f(260) = 0.911099\n",
      ">12 f(259) = 0.911099\n",
      ">13 f(263) = 0.911099\n",
      ">14 f(255) = 0.911099\n",
      ">15 f(255) = 0.911099\n",
      ">16 f(258) = 0.911099\n",
      ">17 f(260) = 0.911099\n",
      ">18 f(260) = 0.911099\n",
      ">19 f(266) = 0.911099\n",
      ">20 f(261) = 0.911099\n",
      ">21 f(263) = 0.911099\n",
      ">22 f(263) = 0.911099\n",
      ">23 f(253) = 0.911099\n",
      ">24 f(260) = 0.911099\n",
      ">25 f(254) = 0.911099\n",
      ">26 f(262) = 0.911099\n",
      ">27 f(260) = 0.911200\n",
      ">28 f(260) = 0.911200\n",
      ">29 f(261) = 0.911200\n",
      ">30 f(258) = 0.911200\n",
      ">31 f(260) = 0.911200\n",
      ">32 f(264) = 0.912300\n",
      ">33 f(265) = 0.912300\n",
      ">34 f(265) = 0.912399\n",
      ">35 f(263) = 0.912399\n",
      ">36 f(267) = 0.912399\n",
      ">37 f(262) = 0.912399\n",
      ">38 f(260) = 0.912399\n",
      ">39 f(265) = 0.912399\n",
      ">40 f(262) = 0.912399\n",
      ">41 f(261) = 0.912399\n",
      ">42 f(261) = 0.912399\n",
      ">43 f(261) = 0.912399\n",
      ">44 f(272) = 0.912399\n",
      ">45 f(266) = 0.912399\n",
      ">46 f(262) = 0.912399\n",
      ">47 f(260) = 0.915200\n",
      ">48 f(259) = 0.915200\n",
      ">49 f(254) = 0.915200\n",
      ">50 f(260) = 0.915200\n",
      ">51 f(259) = 0.915200\n",
      ">52 f(255) = 0.915200\n",
      ">53 f(262) = 0.915200\n",
      ">54 f(253) = 0.915200\n",
      ">55 f(261) = 0.915200\n",
      ">56 f(261) = 0.915200\n",
      ">57 f(260) = 0.915200\n",
      ">58 f(258) = 0.915699\n",
      ">59 f(261) = 0.915699\n",
      ">60 f(255) = 0.915699\n",
      ">61 f(258) = 0.915699\n",
      ">62 f(258) = 0.915699\n",
      ">63 f(255) = 0.915699\n",
      ">64 f(256) = 0.915699\n",
      ">65 f(257) = 0.915699\n",
      ">66 f(260) = 0.915699\n",
      ">67 f(256) = 0.915699\n",
      ">68 f(256) = 0.915699\n",
      ">69 f(258) = 0.915699\n",
      ">70 f(255) = 0.915699\n",
      ">71 f(263) = 0.915699\n",
      ">72 f(254) = 0.915699\n",
      ">73 f(263) = 0.915699\n",
      ">74 f(261) = 0.915699\n",
      ">75 f(253) = 0.915699\n",
      ">76 f(263) = 0.915699\n",
      ">77 f(258) = 0.915699\n",
      ">78 f(253) = 0.915699\n",
      ">79 f(257) = 0.915699\n",
      ">80 f(256) = 0.915699\n",
      ">81 f(254) = 0.915699\n",
      ">82 f(258) = 0.915699\n",
      ">83 f(258) = 0.915699\n",
      ">84 f(250) = 0.915699\n",
      ">85 f(259) = 0.915699\n",
      ">86 f(258) = 0.915699\n",
      ">87 f(260) = 0.915699\n",
      ">88 f(256) = 0.915699\n",
      ">89 f(258) = 0.915699\n",
      ">90 f(256) = 0.915699\n",
      ">91 f(254) = 0.915699\n",
      ">92 f(256) = 0.915699\n",
      ">93 f(256) = 0.915699\n",
      ">94 f(256) = 0.915699\n",
      ">95 f(261) = 0.915699\n",
      ">96 f(261) = 0.915699\n",
      ">97 f(263) = 0.915699\n",
      ">98 f(258) = 0.915699\n",
      ">99 f(254) = 0.916200\n",
      ">100 f(252) = 0.916200\n",
      "Done!\n",
      "Best: f(254) = 0.916200\n"
     ]
    }
   ],
   "source": [
    "# stochastic optimization for feature selection\n",
    "from numpy import mean\n",
    "from numpy.random import rand\n",
    "from numpy.random import choice\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# objective function\n",
    "def objective(X, y, subset):\n",
    "    \n",
    "    # convert into column indexes\n",
    "    ix = [i for i, x in enumerate(subset) if x]\n",
    "    \n",
    "    # check for now column (all False)\n",
    "    if len(ix) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # select columns\n",
    "    X_new = X[:, ix]\n",
    "    \n",
    "    # define model\n",
    "    model = DecisionTreeClassifier()\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X_new, y, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    \n",
    "    # summarize scores\n",
    "    result = mean(scores)\n",
    "    return result, ix\n",
    "\n",
    "# mutation operator\n",
    "def mutate(solution, p_mutate):\n",
    "    \n",
    "    # make a copy\n",
    "    child = solution.copy()\n",
    "    for i in range(len(child)):\n",
    "        \n",
    "        # check for a mutation\n",
    "        if rand() < p_mutate:\n",
    "            \n",
    "            # flip the inclusion\n",
    "            child[i] = not child[i]\n",
    "    return child\n",
    "\n",
    "# hill climbing local search algorithm\n",
    "def hillclimbing(X, y, objective, n_iter, p_mutate):\n",
    "    \n",
    "    # generate an initial point\n",
    "    solution = choice([True, False], size=X.shape[1])\n",
    "    \n",
    "    # evaluate the initial point\n",
    "    solution_eval, ix = objective(X, y, solution)\n",
    "    \n",
    "    # run the hill climb\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        # take a step\n",
    "        candidate = mutate(solution, p_mutate)\n",
    "        \n",
    "        # evaluate candidate point\n",
    "        candidate_eval, ix = objective(X, y, candidate)\n",
    "        \n",
    "        # check if we should keep the new point\n",
    "        if candidate_eval >= solution_eval:\n",
    "            \n",
    "            # store the new point\n",
    "            solution, solution_eval = candidate, candidate_eval\n",
    "            \n",
    "        # report progress\n",
    "        print('>%d f(%s) = %f' % (i+1, len(ix), solution_eval))\n",
    "    return solution, solution_eval\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=500, n_informative=10, n_redundant=490, random_state=1)\n",
    "\n",
    "# define the total iterations\n",
    "n_iter = 100\n",
    "\n",
    "# probability of including/excluding a column\n",
    "p_mut = 10.0 / 500.0\n",
    "\n",
    "# perform the hill climbing search\n",
    "subset, score = hillclimbing(X, y, objective, n_iter, p_mut)\n",
    "\n",
    "# convert into column indexes\n",
    "ix = [i for i, x in enumerate(subset) if x]\n",
    "print('Done!')\n",
    "print('Best: f(%d) = %f' % (len(ix), score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
