{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# define a regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=0.2, random_state=1)\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 7296.390153\n"
     ]
    }
   ],
   "source": [
    "# linear regression model\n",
    "from numpy.random import rand\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# linear regression\n",
    "def predict_row(row, coefficients):\n",
    "    \n",
    "    # add the bias, the last coefficient\n",
    "    result = coefficients[-1]\n",
    "    \n",
    "    # add the weighted input\n",
    "    for i in range(len(row)):\n",
    "        result += coefficients[i] * row[i]\n",
    "    return result\n",
    "\n",
    "# use model coefficients to generate predictions for a dataset of rows\n",
    "def predict_dataset(X, coefficients):\n",
    "    yhats = list()\n",
    "    for row in X:\n",
    "        \n",
    "        # make a prediction\n",
    "        yhat = predict_row(row, coefficients)\n",
    "        \n",
    "        # store the prediction\n",
    "        yhats.append(yhat)\n",
    "    return yhats\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=0.2, random_state=1)\n",
    "\n",
    "# determine the number of coefficients\n",
    "n_coeff = X.shape[1] + 1\n",
    "\n",
    "# generate random coefficients\n",
    "coefficients = rand(n_coeff)\n",
    "\n",
    "# generate predictions for dataset\n",
    "yhat = predict_dataset(X, coefficients)\n",
    "\n",
    "# calculate model prediction error\n",
    "score = mean_squared_error(y, yhat)\n",
    "print('MSE: %f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">5 7378.57413\n",
      ">6 7356.86041\n",
      ">8 7356.65280\n",
      ">9 7316.11649\n",
      ">12 7294.33960\n",
      ">13 7277.51223\n",
      ">20 7276.45759\n",
      ">21 7255.72844\n",
      ">22 7254.81587\n",
      ">23 7254.14810\n",
      ">24 7227.67081\n",
      ">26 7225.23028\n",
      ">31 7223.66274\n",
      ">32 7212.84607\n",
      ">35 7207.47937\n",
      ">40 7165.15904\n",
      ">43 7101.18352\n",
      ">45 7078.20664\n",
      ">46 7059.61796\n",
      ">47 7009.60745\n",
      ">48 6981.69014\n",
      ">50 6973.32747\n",
      ">52 6929.38794\n",
      ">56 6871.33980\n",
      ">60 6860.89554\n",
      ">63 6846.13446\n",
      ">68 6835.47392\n",
      ">70 6832.55287\n",
      ">71 6805.17999\n",
      ">75 6774.50037\n",
      ">78 6759.46282\n",
      ">79 6751.73044\n",
      ">80 6728.92342\n",
      ">83 6710.09460\n",
      ">85 6709.39747\n",
      ">87 6704.73461\n",
      ">88 6663.02829\n",
      ">90 6636.69759\n",
      ">93 6634.82940\n",
      ">95 6626.18580\n",
      ">99 6604.15483\n",
      ">100 6598.27106\n",
      ">102 6592.43709\n",
      ">104 6572.62780\n",
      ">105 6521.18622\n",
      ">112 6500.06122\n",
      ">114 6485.00919\n",
      ">116 6481.79072\n",
      ">118 6467.51063\n",
      ">119 6449.33841\n",
      ">120 6443.88038\n",
      ">121 6436.00870\n",
      ">123 6411.57259\n",
      ">124 6392.30145\n",
      ">126 6351.02070\n",
      ">127 6344.20463\n",
      ">129 6287.83975\n",
      ">130 6265.31320\n",
      ">131 6264.14320\n",
      ">134 6261.76430\n",
      ">137 6216.06039\n",
      ">140 6203.19404\n",
      ">141 6201.59920\n",
      ">143 6147.73211\n",
      ">146 6142.60907\n",
      ">148 6139.96465\n",
      ">150 6122.60321\n",
      ">151 6084.65784\n",
      ">152 6063.12958\n",
      ">153 6028.29621\n",
      ">155 6000.84549\n",
      ">157 5947.51298\n",
      ">160 5942.95870\n",
      ">164 5871.42857\n",
      ">165 5867.14761\n",
      ">167 5833.84481\n",
      ">168 5816.75036\n",
      ">169 5816.65070\n",
      ">170 5800.28406\n",
      ">171 5793.29364\n",
      ">173 5758.09707\n",
      ">174 5725.48765\n",
      ">175 5725.15257\n",
      ">176 5714.23931\n",
      ">178 5700.58523\n",
      ">179 5700.22861\n",
      ">180 5699.58604\n",
      ">181 5653.89877\n",
      ">182 5645.52533\n",
      ">183 5630.33276\n",
      ">184 5619.03596\n",
      ">187 5618.20803\n",
      ">191 5603.35695\n",
      ">192 5595.26677\n",
      ">195 5577.61312\n",
      ">197 5564.98009\n",
      ">202 5538.72054\n",
      ">203 5522.42832\n",
      ">205 5522.25901\n",
      ">208 5513.35355\n",
      ">210 5482.23332\n",
      ">211 5477.31021\n",
      ">212 5472.92572\n",
      ">215 5461.90801\n",
      ">219 5440.64747\n",
      ">222 5412.23880\n",
      ">223 5375.53977\n",
      ">224 5349.89301\n",
      ">225 5349.08865\n",
      ">226 5295.45197\n",
      ">227 5279.32858\n",
      ">228 5264.46331\n",
      ">229 5260.58304\n",
      ">230 5252.05989\n",
      ">231 5236.67077\n",
      ">232 5215.52350\n",
      ">233 5192.84368\n",
      ">235 5157.25097\n",
      ">237 5136.47831\n",
      ">238 5114.69567\n",
      ">239 5079.24699\n",
      ">240 5077.73787\n",
      ">241 5067.48765\n",
      ">244 5048.46402\n",
      ">245 5021.16643\n",
      ">246 5004.49595\n",
      ">247 4976.66493\n",
      ">252 4967.84312\n",
      ">253 4962.91671\n",
      ">257 4948.60932\n",
      ">258 4946.87813\n",
      ">259 4936.55288\n",
      ">260 4935.71477\n",
      ">261 4904.79425\n",
      ">264 4880.34188\n",
      ">265 4844.90724\n",
      ">266 4833.01106\n",
      ">268 4832.87031\n",
      ">269 4824.91468\n",
      ">271 4816.00945\n",
      ">272 4815.10213\n",
      ">277 4806.73688\n",
      ">278 4799.81190\n",
      ">279 4786.46077\n",
      ">280 4786.00392\n",
      ">283 4782.81998\n",
      ">284 4763.20766\n",
      ">285 4754.57566\n",
      ">289 4742.96063\n",
      ">292 4715.40954\n",
      ">297 4690.81051\n",
      ">301 4676.86276\n",
      ">302 4676.05600\n",
      ">305 4663.08375\n",
      ">306 4656.82767\n",
      ">310 4646.86798\n",
      ">311 4611.17415\n",
      ">313 4603.43666\n",
      ">317 4578.17393\n",
      ">318 4563.37310\n",
      ">319 4559.17123\n",
      ">320 4540.49759\n",
      ">326 4536.15827\n",
      ">327 4509.93368\n",
      ">333 4491.97423\n",
      ">334 4490.77796\n",
      ">335 4484.32821\n",
      ">337 4476.51281\n",
      ">338 4466.01606\n",
      ">340 4452.93946\n",
      ">343 4417.04289\n",
      ">345 4406.25960\n",
      ">348 4403.76797\n",
      ">349 4374.41905\n",
      ">350 4361.65558\n",
      ">354 4351.23492\n",
      ">355 4348.33572\n",
      ">357 4334.94658\n",
      ">358 4334.74119\n",
      ">359 4334.23280\n",
      ">360 4304.69518\n",
      ">361 4293.35323\n",
      ">362 4288.80271\n",
      ">364 4281.61749\n",
      ">366 4261.20018\n",
      ">367 4229.98362\n",
      ">368 4207.53474\n",
      ">372 4192.12943\n",
      ">374 4182.38858\n",
      ">376 4170.38104\n",
      ">378 4169.92166\n",
      ">381 4159.40717\n",
      ">382 4149.13059\n",
      ">384 4147.68981\n",
      ">386 4146.30997\n",
      ">392 4134.39533\n",
      ">393 4130.00133\n",
      ">394 4099.09027\n",
      ">395 4095.85321\n",
      ">397 4078.65548\n",
      ">398 4061.19943\n",
      ">399 4053.44908\n",
      ">403 4052.67984\n",
      ">404 4049.94202\n",
      ">405 4048.50664\n",
      ">406 4031.80457\n",
      ">407 4014.77656\n",
      ">409 4013.62246\n",
      ">410 3984.38191\n",
      ">411 3977.88944\n",
      ">412 3968.75865\n",
      ">414 3968.57112\n",
      ">415 3959.73087\n",
      ">417 3944.05374\n",
      ">418 3931.99353\n",
      ">422 3913.79596\n",
      ">426 3901.38471\n",
      ">428 3876.27421\n",
      ">430 3868.46328\n",
      ">431 3863.54681\n",
      ">434 3859.86726\n",
      ">438 3846.41800\n",
      ">439 3829.70680\n",
      ">441 3806.88196\n",
      ">444 3796.08352\n",
      ">447 3776.38103\n",
      ">448 3756.98173\n",
      ">450 3734.44289\n",
      ">452 3709.97813\n",
      ">456 3690.53347\n",
      ">457 3675.29564\n",
      ">459 3666.77979\n",
      ">460 3662.93885\n",
      ">463 3654.40117\n",
      ">467 3653.75711\n",
      ">468 3634.07621\n",
      ">469 3610.48084\n",
      ">470 3603.47627\n",
      ">472 3590.52581\n",
      ">474 3576.32714\n",
      ">475 3564.01372\n",
      ">480 3546.42721\n",
      ">481 3492.12521\n",
      ">482 3475.92401\n",
      ">484 3467.82894\n",
      ">490 3454.02501\n",
      ">492 3441.80122\n",
      ">496 3427.06093\n",
      ">498 3424.18823\n",
      ">500 3387.54670\n",
      ">502 3386.07801\n",
      ">506 3368.98979\n",
      ">507 3340.87814\n",
      ">511 3337.78499\n",
      ">512 3317.40712\n",
      ">513 3315.83518\n",
      ">514 3310.03859\n",
      ">515 3298.70219\n",
      ">517 3288.06282\n",
      ">519 3285.54628\n",
      ">520 3278.98647\n",
      ">521 3234.96265\n",
      ">522 3219.95719\n",
      ">523 3205.20258\n",
      ">527 3194.63107\n",
      ">534 3180.27868\n",
      ">535 3177.66064\n",
      ">537 3164.55910\n",
      ">538 3163.35953\n",
      ">539 3156.09323\n",
      ">541 3128.42294\n",
      ">543 3105.36471\n",
      ">546 3053.82279\n",
      ">548 3039.64835\n",
      ">550 3035.16255\n",
      ">551 3008.22897\n",
      ">552 3000.09817\n",
      ">557 2989.71133\n",
      ">558 2985.50223\n",
      ">559 2969.20734\n",
      ">560 2969.12122\n",
      ">564 2944.51671\n",
      ">566 2919.71032\n",
      ">568 2919.42498\n",
      ">569 2899.00285\n",
      ">570 2878.51317\n",
      ">572 2864.95069\n",
      ">573 2833.14924\n",
      ">575 2831.10701\n",
      ">577 2830.45054\n",
      ">583 2826.84965\n",
      ">584 2796.08258\n",
      ">585 2783.93864\n",
      ">586 2776.14646\n",
      ">587 2768.40280\n",
      ">590 2762.81838\n",
      ">595 2761.86469\n",
      ">597 2749.40517\n",
      ">599 2732.35716\n",
      ">601 2725.02307\n",
      ">602 2700.53526\n",
      ">608 2690.87281\n",
      ">609 2685.38719\n",
      ">611 2662.52376\n",
      ">612 2655.23130\n",
      ">616 2636.73684\n",
      ">618 2624.70212\n",
      ">624 2619.59211\n",
      ">626 2603.55452\n",
      ">630 2594.23319\n",
      ">631 2578.17910\n",
      ">632 2567.09357\n",
      ">633 2556.71341\n",
      ">634 2553.74967\n",
      ">639 2553.42113\n",
      ">641 2535.20384\n",
      ">642 2533.09404\n",
      ">643 2527.81462\n",
      ">645 2525.31723\n",
      ">646 2521.81053\n",
      ">650 2504.15297\n",
      ">651 2478.79512\n",
      ">654 2478.25658\n",
      ">660 2475.93670\n",
      ">662 2451.81146\n",
      ">663 2449.66632\n",
      ">664 2445.15447\n",
      ">667 2444.82352\n",
      ">669 2435.58031\n",
      ">672 2429.26027\n",
      ">675 2428.75652\n",
      ">676 2426.81261\n",
      ">679 2407.91282\n",
      ">681 2402.44810\n",
      ">682 2363.30693\n",
      ">683 2350.22517\n",
      ">685 2348.66027\n",
      ">687 2328.85326\n",
      ">688 2321.29761\n",
      ">690 2313.87198\n",
      ">691 2312.10602\n",
      ">694 2302.78073\n",
      ">695 2302.63667\n",
      ">697 2298.83039\n",
      ">699 2285.76497\n",
      ">700 2274.70227\n",
      ">701 2257.97381\n",
      ">703 2244.64604\n",
      ">704 2226.73037\n",
      ">705 2225.36002\n",
      ">706 2207.08114\n",
      ">707 2190.04908\n",
      ">708 2164.37293\n",
      ">710 2155.92892\n",
      ">712 2154.66440\n",
      ">715 2116.61377\n",
      ">716 2108.30629\n",
      ">717 2105.48703\n",
      ">719 2093.98542\n",
      ">721 2093.46850\n",
      ">722 2091.80987\n",
      ">723 2075.20242\n",
      ">724 2071.00816\n",
      ">725 2063.94624\n",
      ">726 2061.98238\n",
      ">727 2051.10469\n",
      ">728 2045.59972\n",
      ">730 2040.73666\n",
      ">731 2040.13968\n",
      ">732 2029.57675\n",
      ">733 2021.00998\n",
      ">737 2019.88879\n",
      ">741 2002.26895\n",
      ">747 1994.26381\n",
      ">748 1992.72805\n",
      ">749 1987.78123\n",
      ">750 1975.70276\n",
      ">752 1955.67876\n",
      ">753 1945.22019\n",
      ">756 1925.06262\n",
      ">758 1902.70864\n",
      ">761 1897.64339\n",
      ">762 1863.67000\n",
      ">765 1852.60339\n",
      ">766 1828.20330\n",
      ">767 1822.18386\n",
      ">769 1818.60822\n",
      ">773 1812.82376\n",
      ">775 1805.08715\n",
      ">777 1800.00479\n",
      ">778 1782.50408\n",
      ">781 1767.26756\n",
      ">783 1762.99866\n",
      ">786 1762.36350\n",
      ">790 1754.19744\n",
      ">792 1739.32928\n",
      ">793 1714.39753\n",
      ">794 1705.00453\n",
      ">795 1703.03887\n",
      ">801 1693.51584\n",
      ">809 1686.04744\n",
      ">811 1682.50105\n",
      ">812 1681.86068\n",
      ">816 1674.57583\n",
      ">818 1666.13664\n",
      ">820 1661.21268\n",
      ">823 1639.51055\n",
      ">828 1633.26360\n",
      ">835 1613.21601\n",
      ">836 1598.69945\n",
      ">837 1573.73654\n",
      ">839 1569.93008\n",
      ">840 1536.22977\n",
      ">842 1522.87365\n",
      ">843 1521.73017\n",
      ">848 1509.04818\n",
      ">852 1496.30949\n",
      ">853 1487.75306\n",
      ">858 1486.16997\n",
      ">859 1470.88290\n",
      ">860 1467.73668\n",
      ">862 1462.78311\n",
      ">864 1454.55296\n",
      ">865 1435.22696\n",
      ">866 1432.93288\n",
      ">867 1427.44702\n",
      ">869 1417.67197\n",
      ">872 1402.75406\n",
      ">874 1389.30570\n",
      ">875 1371.70860\n",
      ">877 1362.79236\n",
      ">881 1355.39954\n",
      ">884 1352.90778\n",
      ">885 1351.24881\n",
      ">886 1332.23217\n",
      ">887 1331.39320\n",
      ">888 1325.09625\n",
      ">889 1323.00788\n",
      ">890 1318.34939\n",
      ">892 1308.48311\n",
      ">894 1304.31475\n",
      ">895 1294.78300\n",
      ">896 1289.70218\n",
      ">899 1271.84716\n",
      ">901 1268.69924\n",
      ">904 1267.34843\n",
      ">906 1266.56930\n",
      ">907 1260.82827\n",
      ">910 1253.43449\n",
      ">913 1246.67894\n",
      ">919 1234.92723\n",
      ">921 1223.05160\n",
      ">922 1210.10487\n",
      ">923 1204.54354\n",
      ">924 1199.15501\n",
      ">925 1191.98543\n",
      ">927 1191.75240\n",
      ">932 1189.76389\n",
      ">933 1185.32331\n",
      ">935 1176.48636\n",
      ">936 1170.86602\n",
      ">937 1157.56445\n",
      ">938 1150.06338\n",
      ">939 1138.91931\n",
      ">940 1132.69940\n",
      ">946 1117.99184\n",
      ">947 1115.17590\n",
      ">948 1110.63321\n",
      ">949 1091.81603\n",
      ">951 1086.17380\n",
      ">963 1086.06352\n",
      ">964 1085.02951\n",
      ">965 1078.88741\n",
      ">968 1065.36743\n",
      ">970 1052.42947\n",
      ">973 1051.72795\n",
      ">974 1049.37670\n",
      ">975 1042.24396\n",
      ">980 1023.42687\n",
      ">981 1007.19548\n",
      ">982 993.33337\n",
      ">984 987.74040\n",
      ">985 978.07099\n",
      ">988 975.64250\n",
      ">990 970.94456\n",
      ">991 958.78900\n",
      ">993 949.71863\n",
      ">994 949.66809\n",
      ">996 942.22403\n",
      ">997 938.32538\n",
      ">998 933.33595\n",
      ">1001 927.25739\n",
      ">1002 916.94556\n",
      ">1006 915.78688\n",
      ">1010 900.92677\n",
      ">1012 885.32258\n",
      ">1015 883.54335\n",
      ">1016 882.76612\n",
      ">1017 878.25594\n",
      ">1021 877.01916\n",
      ">1022 876.21579\n",
      ">1027 871.58845\n",
      ">1028 870.34850\n",
      ">1029 865.53338\n",
      ">1038 863.37869\n",
      ">1042 856.48339\n",
      ">1045 835.27802\n",
      ">1047 835.24257\n",
      ">1048 826.26751\n",
      ">1052 816.88797\n",
      ">1054 814.86892\n",
      ">1056 802.01962\n",
      ">1057 792.30671\n",
      ">1061 778.54243\n",
      ">1064 773.78860\n",
      ">1065 772.03269\n",
      ">1071 765.35644\n",
      ">1075 751.68303\n",
      ">1078 751.02464\n",
      ">1079 747.22132\n",
      ">1086 742.39872\n",
      ">1088 740.89017\n",
      ">1089 736.75626\n",
      ">1091 734.78172\n",
      ">1092 734.35052\n",
      ">1097 722.94313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1099 720.05102\n",
      ">1100 719.28255\n",
      ">1102 715.67672\n",
      ">1104 706.36367\n",
      ">1105 701.38071\n",
      ">1106 694.88570\n",
      ">1109 686.67586\n",
      ">1110 678.48441\n",
      ">1111 674.71372\n",
      ">1112 664.30305\n",
      ">1113 660.17013\n",
      ">1116 654.75363\n",
      ">1117 654.09347\n",
      ">1119 651.80540\n",
      ">1120 651.14868\n",
      ">1121 645.10050\n",
      ">1122 627.72966\n",
      ">1127 625.47245\n",
      ">1128 622.71429\n",
      ">1130 621.74795\n",
      ">1131 607.89596\n",
      ">1133 604.41709\n",
      ">1134 594.76361\n",
      ">1135 594.19162\n",
      ">1138 590.24272\n",
      ">1140 588.38063\n",
      ">1141 577.51150\n",
      ">1142 575.30178\n",
      ">1144 567.89622\n",
      ">1147 563.50499\n",
      ">1152 560.38727\n",
      ">1153 555.16551\n",
      ">1154 553.46982\n",
      ">1155 544.16742\n",
      ">1156 532.86996\n",
      ">1157 532.08834\n",
      ">1160 526.62482\n",
      ">1162 523.91207\n",
      ">1163 520.77670\n",
      ">1164 519.01740\n",
      ">1165 516.20067\n",
      ">1169 511.78358\n",
      ">1171 503.13948\n",
      ">1172 501.72215\n",
      ">1174 491.60056\n",
      ">1176 479.56166\n",
      ">1177 468.42913\n",
      ">1178 459.56647\n",
      ">1179 456.27247\n",
      ">1182 445.14537\n",
      ">1183 443.64814\n",
      ">1184 438.09571\n",
      ">1188 436.25422\n",
      ">1189 431.26007\n",
      ">1192 426.44020\n",
      ">1195 422.63821\n",
      ">1196 417.07897\n",
      ">1197 406.23665\n",
      ">1199 395.03621\n",
      ">1201 394.61072\n",
      ">1205 393.20172\n",
      ">1209 393.03139\n",
      ">1210 392.88099\n",
      ">1212 381.67550\n",
      ">1214 378.20731\n",
      ">1215 375.25248\n",
      ">1221 373.97406\n",
      ">1222 373.69362\n",
      ">1224 372.80627\n",
      ">1225 366.59363\n",
      ">1226 364.24461\n",
      ">1227 355.80611\n",
      ">1228 352.51294\n",
      ">1229 348.43444\n",
      ">1230 344.38607\n",
      ">1236 341.67222\n",
      ">1241 335.78880\n",
      ">1244 333.70659\n",
      ">1246 330.05830\n",
      ">1247 326.06199\n",
      ">1248 325.10327\n",
      ">1252 323.01807\n",
      ">1253 313.03035\n",
      ">1255 311.57177\n",
      ">1257 310.63858\n",
      ">1260 301.70257\n",
      ">1261 301.15425\n",
      ">1263 298.95466\n",
      ">1264 293.71946\n",
      ">1265 286.51871\n",
      ">1267 286.27214\n",
      ">1268 281.94781\n",
      ">1270 277.11012\n",
      ">1273 273.25713\n",
      ">1274 268.11514\n",
      ">1275 266.99299\n",
      ">1278 258.86013\n",
      ">1280 257.81006\n",
      ">1283 254.94901\n",
      ">1284 254.10520\n",
      ">1288 251.52438\n",
      ">1290 249.69585\n",
      ">1291 245.06550\n",
      ">1293 235.88677\n",
      ">1294 232.37178\n",
      ">1296 229.79897\n",
      ">1298 227.03522\n",
      ">1300 224.20549\n",
      ">1301 222.53628\n",
      ">1302 220.63681\n",
      ">1303 212.41656\n",
      ">1304 210.05399\n",
      ">1311 202.34337\n",
      ">1314 200.43594\n",
      ">1315 190.78115\n",
      ">1317 190.19076\n",
      ">1318 189.70737\n",
      ">1323 186.06905\n",
      ">1328 183.18315\n",
      ">1329 181.55643\n",
      ">1330 178.79944\n",
      ">1331 172.63553\n",
      ">1334 169.53031\n",
      ">1337 168.37909\n",
      ">1338 166.29489\n",
      ">1339 164.47273\n",
      ">1340 163.73186\n",
      ">1342 163.56030\n",
      ">1345 158.13123\n",
      ">1346 157.22297\n",
      ">1347 152.43884\n",
      ">1348 151.83881\n",
      ">1351 147.33292\n",
      ">1354 146.85992\n",
      ">1355 145.15366\n",
      ">1356 141.11426\n",
      ">1358 137.63998\n",
      ">1359 133.86943\n",
      ">1362 133.49596\n",
      ">1363 132.04349\n",
      ">1366 128.56332\n",
      ">1367 128.01428\n",
      ">1369 127.10661\n",
      ">1370 124.54761\n",
      ">1371 121.28102\n",
      ">1372 118.10933\n",
      ">1374 117.91802\n",
      ">1375 116.53592\n",
      ">1376 113.92047\n",
      ">1377 111.27898\n",
      ">1382 110.02393\n",
      ">1383 108.07161\n",
      ">1386 108.05346\n",
      ">1387 107.44878\n",
      ">1390 106.26407\n",
      ">1393 104.79798\n",
      ">1395 99.63889\n",
      ">1397 99.29711\n",
      ">1399 95.91260\n",
      ">1401 92.44341\n",
      ">1402 91.34608\n",
      ">1403 89.22567\n",
      ">1405 86.36494\n",
      ">1406 82.08473\n",
      ">1408 81.78528\n",
      ">1409 78.79942\n",
      ">1412 78.63799\n",
      ">1414 77.01096\n",
      ">1418 74.14227\n",
      ">1419 72.42695\n",
      ">1420 71.93551\n",
      ">1422 69.40366\n",
      ">1423 68.16152\n",
      ">1424 65.89153\n",
      ">1426 64.78690\n",
      ">1431 61.50576\n",
      ">1435 59.63298\n",
      ">1436 58.87049\n",
      ">1438 56.52846\n",
      ">1442 54.80781\n",
      ">1443 54.66969\n",
      ">1444 54.11773\n",
      ">1445 53.30988\n",
      ">1446 48.90214\n",
      ">1447 47.31611\n",
      ">1450 43.61843\n",
      ">1457 40.86604\n",
      ">1458 40.76342\n",
      ">1459 39.86561\n",
      ">1460 39.00584\n",
      ">1461 36.61387\n",
      ">1463 35.97553\n",
      ">1469 35.64380\n",
      ">1471 34.41408\n",
      ">1473 33.98686\n",
      ">1474 31.91943\n",
      ">1476 30.80768\n",
      ">1477 29.01203\n",
      ">1481 28.65364\n",
      ">1482 27.14962\n",
      ">1483 26.41525\n",
      ">1485 25.94194\n",
      ">1486 25.54184\n",
      ">1487 23.38703\n",
      ">1488 22.21959\n",
      ">1492 21.63503\n",
      ">1493 21.18397\n",
      ">1494 21.17328\n",
      ">1496 19.54299\n",
      ">1500 17.69846\n",
      ">1502 17.32375\n",
      ">1505 16.54976\n",
      ">1507 15.27272\n",
      ">1508 14.35493\n",
      ">1510 13.80927\n",
      ">1511 13.50978\n",
      ">1513 12.73453\n",
      ">1514 12.27161\n",
      ">1518 10.97595\n",
      ">1525 10.37851\n",
      ">1527 10.12651\n",
      ">1533 9.53036\n",
      ">1534 8.54799\n",
      ">1535 8.17508\n",
      ">1537 8.07091\n",
      ">1541 7.24894\n",
      ">1542 6.92694\n",
      ">1543 6.65126\n",
      ">1547 6.33917\n",
      ">1548 6.08535\n",
      ">1550 6.01546\n",
      ">1552 4.77152\n",
      ">1554 4.70982\n",
      ">1558 3.83848\n",
      ">1559 3.59368\n",
      ">1561 3.34571\n",
      ">1564 3.22993\n",
      ">1566 3.06499\n",
      ">1570 2.85234\n",
      ">1571 2.44582\n",
      ">1576 2.09372\n",
      ">1577 2.02091\n",
      ">1578 1.81047\n",
      ">1583 1.41405\n",
      ">1589 1.29040\n",
      ">1591 1.18931\n",
      ">1596 1.07144\n",
      ">1600 1.07047\n",
      ">1603 0.65172\n",
      ">1605 0.48591\n",
      ">1607 0.41250\n",
      ">1610 0.39490\n",
      ">1612 0.30484\n",
      ">1617 0.16108\n",
      ">1661 0.14107\n",
      ">1683 0.13361\n",
      ">1712 0.09873\n",
      "Done!\n",
      "Coefficients: [ 1.48085550e-01  3.11999619e-02  3.24753454e+00  1.15773496e-01\n",
      " -4.59622072e-03  8.63644916e+01  1.68388152e-02  3.93846169e-02\n",
      " -3.61185262e-02  7.49690418e-02 -2.09802588e-02]\n",
      "Train MSE: 0.098731\n",
      "Test MSE: 0.109807\n"
     ]
    }
   ],
   "source": [
    "# optimize linear regression coefficients for regression dataset\n",
    "from numpy.random import randn\n",
    "from numpy.random import rand\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# linear regression\n",
    "def predict_row(row, coefficients):\n",
    "    \n",
    "    # add the bias, the last coefficient\n",
    "    result = coefficients[-1]\n",
    "    \n",
    "    # add the weighted input\n",
    "    for i in range(len(row)):\n",
    "        result += coefficients[i] * row[i]\n",
    "    return result\n",
    "\n",
    "# use model coefficients to generate predictions for a dataset of rows\n",
    "def predict_dataset(X, coefficients):\n",
    "    yhats = list()\n",
    "    for row in X:\n",
    "        \n",
    "        # make a prediction\n",
    "        yhat = predict_row(row, coefficients)\n",
    "        \n",
    "        # store the prediction\n",
    "        yhats.append(yhat)\n",
    "    return yhats\n",
    "\n",
    "# objective function\n",
    "def objective(X, y, coefficients):\n",
    "    \n",
    "    # generate predictions for dataset\n",
    "    yhat = predict_dataset(X, coefficients)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    score = mean_squared_error(y, yhat)\n",
    "    return score\n",
    "\n",
    "# hill climbing local search algorithm\n",
    "def hillclimbing(X, y, objective, solution, n_iter, step_size):\n",
    "    \n",
    "    # evaluate the initial point\n",
    "    solution_eval = objective(X, y, solution)\n",
    "    \n",
    "    # run the hill climb\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        # take a step\n",
    "        candidate = solution + randn(len(solution)) * step_size\n",
    "        \n",
    "        # evaluate candidate point\n",
    "        candidte_eval = objective(X, y, candidate)\n",
    "        \n",
    "        # check if we should keep the new point\n",
    "        if candidte_eval <= solution_eval:\n",
    "            \n",
    "            # store the new point\n",
    "            solution, solution_eval = candidate, candidte_eval\n",
    "            \n",
    "            # report progress\n",
    "            print('>%d %.5f' % (i, solution_eval))\n",
    "    return [solution, solution_eval]\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=0.2, random_state=1)\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# define the total iterations\n",
    "n_iter = 2000\n",
    "\n",
    "# define the maximum step size\n",
    "step_size = 0.15\n",
    "\n",
    "# determine the number of coefficients\n",
    "n_coef = X.shape[1] + 1\n",
    "\n",
    "# define the initial solution\n",
    "solution = rand(n_coef)\n",
    "\n",
    "# perform the hill climbing search\n",
    "coefficients, score = hillclimbing(X_train, y_train, objective, solution, n_iter, step_size)\n",
    "print('Done!')\n",
    "print('Coefficients: %s' % coefficients)\n",
    "print('Train MSE: %f' % (score))\n",
    "\n",
    "# generate predictions for the test dataset\n",
    "yhat = predict_dataset(X_test, coefficients)\n",
    "\n",
    "# calculate accuracy\n",
    "score = mean_squared_error(y_test, yhat)\n",
    "print('Test MSE: %f' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# define a binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.696000\n"
     ]
    }
   ],
   "source": [
    "# logistic regression function for binary classification\n",
    "from math import exp\n",
    "from numpy.random import rand\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# logistic regression\n",
    "def predict_row(row, coefficients):\n",
    "    \n",
    "    # add the bias, the last coefficient\n",
    "    result = coefficients[-1]\n",
    "    \n",
    "    # add the weighted input\n",
    "    for i in range(len(row)):\n",
    "        result += coefficients[i] * row[i]\n",
    "        \n",
    "    # logistic function\n",
    "    logistic = 1.0 / (1.0 + exp(-result))\n",
    "    return logistic\n",
    "\n",
    "# use model coefficients to generate predictions for a dataset of rows\n",
    "def predict_dataset(X, coefficients):\n",
    "    yhats = list()\n",
    "    for row in X:\n",
    "        \n",
    "        # make a prediction\n",
    "        yhat = predict_row(row, coefficients)\n",
    "        \n",
    "        # store the prediction\n",
    "        yhats.append(yhat)\n",
    "    return yhats\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
    "\n",
    "# determine the number of coefficients\n",
    "n_coeff = X.shape[1] + 1\n",
    "\n",
    "# generate random coefficients\n",
    "coefficients = rand(n_coeff)\n",
    "\n",
    "# generate predictions for dataset\n",
    "yhat = predict_dataset(X, coefficients)\n",
    "\n",
    "# round predictions to labels\n",
    "yhat = [round(y) for y in yhat]\n",
    "\n",
    "# calculate accuracy\n",
    "score = accuracy_score(y, yhat)\n",
    "print('Accuracy: %f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 0.73582\n",
      ">2 0.75224\n",
      ">3 0.78657\n",
      ">5 0.79104\n",
      ">12 0.80149\n",
      ">14 0.81791\n",
      ">15 0.82090\n",
      ">16 0.82388\n",
      ">21 0.82388\n",
      ">31 0.82985\n",
      ">54 0.82985\n",
      ">57 0.82985\n",
      ">58 0.83433\n",
      ">74 0.84328\n",
      ">78 0.84925\n",
      ">79 0.85075\n",
      ">80 0.85224\n",
      ">90 0.85522\n",
      ">92 0.85672\n",
      ">184 0.85672\n",
      ">665 0.85821\n",
      ">770 0.85821\n",
      ">792 0.85821\n",
      ">972 0.85821\n",
      ">1014 0.85821\n",
      ">1024 0.85821\n",
      ">1025 0.85821\n",
      ">1041 0.85970\n",
      ">1237 0.85970\n",
      ">1553 0.85970\n",
      ">1983 0.85970\n",
      "Done!\n",
      "Coefficients: [ 0.32353165  0.22831124  2.39979865 -0.80711649 -0.70372284 -0.00628508]\n",
      "Train Accuracy: 0.859701\n",
      "Test Accuracy: 0.839394\n"
     ]
    }
   ],
   "source": [
    "# optimize logistic regression model with a stochastic hill climber\n",
    "from math import exp\n",
    "from numpy.random import randn\n",
    "from numpy.random import rand\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# logistic regression\n",
    "def predict_row(row, coefficients):\n",
    "    # add the bias, the last coefficient\n",
    "    result = coefficients[-1]\n",
    "    # add the weighted input\n",
    "    for i in range(len(row)):\n",
    "        result += coefficients[i] * row[i]\n",
    "    # logistic function\n",
    "    logistic = 1.0 / (1.0 + exp(-result))\n",
    "    return logistic\n",
    "\n",
    "# use model coefficients to generate predictions for a dataset of rows\n",
    "def predict_dataset(X, coefficients):\n",
    "    yhats = list()\n",
    "    for row in X:\n",
    "        # make a prediction\n",
    "        yhat = predict_row(row, coefficients)\n",
    "        # store the prediction\n",
    "        yhats.append(yhat)\n",
    "    return yhats\n",
    "\n",
    "# objective function\n",
    "def objective(X, y, coefficients):\n",
    "    # generate predictions for dataset\n",
    "    yhat = predict_dataset(X, coefficients)\n",
    "    # round predictions to labels\n",
    "    yhat = [round(y) for y in yhat]\n",
    "    # calculate accuracy\n",
    "    score = accuracy_score(y, yhat)\n",
    "    return score\n",
    "\n",
    "# hill climbing local search algorithm\n",
    "def hillclimbing(X, y, objective, solution, n_iter, step_size):\n",
    "    # evaluate the initial point\n",
    "    solution_eval = objective(X, y, solution)\n",
    "    # run the hill climb\n",
    "    for i in range(n_iter):\n",
    "        # take a step\n",
    "        candidate = solution + randn(len(solution)) * step_size\n",
    "        # evaluate candidate point\n",
    "        candidte_eval = objective(X, y, candidate)\n",
    "        # check if we should keep the new point\n",
    "        if candidte_eval >= solution_eval:\n",
    "            # store the new point\n",
    "            solution, solution_eval = candidate, candidte_eval\n",
    "            # report progress\n",
    "            print('>%d %.5f' % (i, solution_eval))\n",
    "    return [solution, solution_eval]\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# define the total iterations\n",
    "n_iter = 2000\n",
    "\n",
    "# define the maximum step size\n",
    "step_size = 0.1\n",
    "\n",
    "# determine the number of coefficients\n",
    "n_coef = X.shape[1] + 1\n",
    "\n",
    "# define the initial solution\n",
    "solution = rand(n_coef)\n",
    "\n",
    "# perform the hill climbing search\n",
    "coefficients, score = hillclimbing(X_train, y_train, objective, solution, n_iter, step_size)\n",
    "print('Done!')\n",
    "print('Coefficients: %s' % coefficients)\n",
    "print('Train Accuracy: %f' % (score))\n",
    "\n",
    "# generate predictions for the test dataset\n",
    "yhat = predict_dataset(X_test, coefficients)\n",
    "\n",
    "# round predictions to labels\n",
    "yhat = [round(y) for y in yhat]\n",
    "\n",
    "# calculate accuracy\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Test Accuracy: %f' % (score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
