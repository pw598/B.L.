{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "\t# select indices of points with the class label\n",
    "\trow_ix = where(y == class_value)\n",
    "\t# scatter plot for points with a different color\n",
    "\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop an mlp for blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models to file toward the end of a training run\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "n_epochs, n_save_after = 500, 490\n",
    "for i in range(n_epochs):\n",
    "\t# fit model for a single epoch\n",
    "\tmodel.fit(trainX, trainy, epochs=1, verbose=0)\n",
    "\t# check if we should save the model\n",
    "\tif i >= n_save_after:\n",
    "\t\tmodel.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the weights of multiple loaded models\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from numpy import average\n",
    "from numpy import array\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_start, n_end):\n",
    "\tall_models = list()\n",
    "\tfor epoch in range(n_start, n_end):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'model_' + str(epoch) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create a model from the weights of multiple models\n",
    "def model_weight_ensemble(members, weights):\n",
    "\t# determine how many layers need to be averaged\n",
    "\tn_layers = len(members[0].get_weights())\n",
    "\t# create an set of average model weights\n",
    "\tavg_model_weights = list()\n",
    "\tfor layer in range(n_layers):\n",
    "\t\t# collect this layer from each model\n",
    "\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n",
    "\t\t# weighted average of weights for this layer\n",
    "\t\tavg_layer_weights = average(layer_weights, axis=0, weights=weights)\n",
    "\t\t# store average layer weights\n",
    "\t\tavg_model_weights.append(avg_layer_weights)\n",
    "\t# create a new model with the same structure\n",
    "\tmodel = clone_model(members[0])\n",
    "\t# set the weights in the new\n",
    "\tmodel.set_weights(avg_model_weights)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# load all models into memory\n",
    "members = load_all_models(490, 500)\n",
    "print('Loaded %d models' % len(members))\n",
    "# prepare an array of equal weights\n",
    "n_models = len(members)\n",
    "weights = [1/n_models for i in range(1, n_models+1)]\n",
    "# create a new model with the weighted average of all model weights\n",
    "model = model_weight_ensemble(members, weights)\n",
    "# summarize the created model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of model weights on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from matplotlib import pyplot\n",
    "from numpy import average\n",
    "from numpy import array\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_start, n_end):\n",
    "\tall_models = list()\n",
    "\tfor epoch in range(n_start, n_end):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'model_' + str(epoch) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# # create a model from the weights of multiple models\n",
    "def model_weight_ensemble(members, weights):\n",
    "\t# determine how many layers need to be averaged\n",
    "\tn_layers = len(members[0].get_weights())\n",
    "\t# create an set of average model weights\n",
    "\tavg_model_weights = list()\n",
    "\tfor layer in range(n_layers):\n",
    "\t\t# collect this layer from each model\n",
    "\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n",
    "\t\t# weighted average of weights for this layer\n",
    "\t\tavg_layer_weights = average(layer_weights, axis=0, weights=weights)\n",
    "\t\t# store average layer weights\n",
    "\t\tavg_model_weights.append(avg_layer_weights)\n",
    "\t# create a new model with the same structure\n",
    "\tmodel = clone_model(members[0])\n",
    "\t# set the weights in the new\n",
    "\tmodel.set_weights(avg_model_weights)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "\t# select a subset of members\n",
    "\tsubset = members[:n_members]\n",
    "\t# prepare an array of equal weights\n",
    "\tweights = [1.0/n_members for i in range(1, n_members+1)]\n",
    "\t# create a new model with the weighted average of all model weights\n",
    "\tmodel = model_weight_ensemble(subset, weights)\n",
    "\t# make predictions and evaluate accuracy\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\treturn test_acc\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# load models in order\n",
    "members = load_all_models(490, 500)\n",
    "print('Loaded %d models' % len(members))\n",
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "\t# evaluate model with i members\n",
    "\tensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\t# evaluate the i'th model standalone\n",
    "\t_, single_score = members[i-1].evaluate(testX, testy, verbose=0)\n",
    "\t# summarize this step\n",
    "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "\tensemble_scores.append(ensemble_score)\n",
    "\tsingle_scores.append(single_score)\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly decreasing weighted average of models on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from matplotlib import pyplot\n",
    "from numpy import average\n",
    "from numpy import array\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_start, n_end):\n",
    "\tall_models = list()\n",
    "\tfor epoch in range(n_start, n_end):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'model_' + str(epoch) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create a model from the weights of multiple models\n",
    "def model_weight_ensemble(members, weights):\n",
    "\t# determine how many layers need to be averaged\n",
    "\tn_layers = len(members[0].get_weights())\n",
    "\t# create an set of average model weights\n",
    "\tavg_model_weights = list()\n",
    "\tfor layer in range(n_layers):\n",
    "\t\t# collect this layer from each model\n",
    "\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n",
    "\t\t# weighted average of weights for this layer\n",
    "\t\tavg_layer_weights = average(layer_weights, axis=0, weights=weights)\n",
    "\t\t# store average layer weights\n",
    "\t\tavg_model_weights.append(avg_layer_weights)\n",
    "\t# create a new model with the same structure\n",
    "\tmodel = clone_model(members[0])\n",
    "\t# set the weights in the new\n",
    "\tmodel.set_weights(avg_model_weights)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "\t# select a subset of members\n",
    "\tsubset = members[:n_members]\n",
    "\t# prepare an array of linearly decreasing weights\n",
    "\tweights = [i/n_members for i in range(n_members, 0, -1)]\n",
    "\t# create a new model with the weighted average of all model weights\n",
    "\tmodel = model_weight_ensemble(subset, weights)\n",
    "\t# make predictions and evaluate accuracy\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\treturn test_acc\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# load models in order\n",
    "members = load_all_models(490, 500)\n",
    "print('Loaded %d models' % len(members))\n",
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "\t# evaluate model with i members\n",
    "\tensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\t# evaluate the i'th model standalone\n",
    "\t_, single_score = members[i-1].evaluate(testX, testy, verbose=0)\n",
    "\t# summarize this step\n",
    "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "\tensemble_scores.append(ensemble_score)\n",
    "\tsingle_scores.append(single_score)\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponentially decreasing weighted average of models on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from matplotlib import pyplot\n",
    "from numpy import average\n",
    "from numpy import array\n",
    "from math import exp\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_start, n_end):\n",
    "\tall_models = list()\n",
    "\tfor epoch in range(n_start, n_end):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'model_' + str(epoch) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create a model from the weights of multiple models\n",
    "def model_weight_ensemble(members, weights):\n",
    "\t# determine how many layers need to be averaged\n",
    "\tn_layers = len(members[0].get_weights())\n",
    "\t# create an set of average model weights\n",
    "\tavg_model_weights = list()\n",
    "\tfor layer in range(n_layers):\n",
    "\t\t# collect this layer from each model\n",
    "\t\tlayer_weights = array([model.get_weights()[layer] for model in members])\n",
    "\t\t# weighted average of weights for this layer\n",
    "\t\tavg_layer_weights = average(layer_weights, axis=0, weights=weights)\n",
    "\t\t# store average layer weights\n",
    "\t\tavg_model_weights.append(avg_layer_weights)\n",
    "\t# create a new model with the same structure\n",
    "\tmodel = clone_model(members[0])\n",
    "\t# set the weights in the new\n",
    "\tmodel.set_weights(avg_model_weights)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "\t# select a subset of members\n",
    "\tsubset = members[:n_members]\n",
    "\t# prepare an array of exponentially decreasing weights\n",
    "\talpha = 2.0\n",
    "\tweights = [exp(-i/alpha) for i in range(1, n_members+1)]\n",
    "\t# create a new model with the weighted average of all model weights\n",
    "\tmodel = model_weight_ensemble(subset, weights)\n",
    "\t# make predictions and evaluate accuracy\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\treturn test_acc\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# load models in order\n",
    "members = load_all_models(490, 500)\n",
    "print('Loaded %d models' % len(members))\n",
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "\t# evaluate model with i members\n",
    "\tensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\t# evaluate the i'th model standalone\n",
    "\t_, single_score = members[i-1].evaluate(testX, testy, verbose=0)\n",
    "\t# summarize this step\n",
    "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "\tensemble_scores.append(ensemble_score)\n",
    "\tsingle_scores.append(single_score)\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
