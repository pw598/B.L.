{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "\t# select indices of points with the class label\n",
    "\trow_ix = where(y == class_value)\n",
    "\t# scatter plot for points with a different color\n",
    "\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop an mlp for blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model averaging ensemble for the blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "\ttrainy_enc = to_categorical(trainy)\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "\t# make predictions\n",
    "\tyhats = [model.predict(testX) for model in members]\n",
    "\tyhats = array(yhats)\n",
    "\t# sum across ensemble members\n",
    "\tsummed = numpy.sum(yhats, axis=0)\n",
    "\t# argmax across classes\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\treturn result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "\t# select a subset of members\n",
    "\tsubset = members[:n_members]\n",
    "\t# make prediction\n",
    "\tyhat = ensemble_predictions(subset, testX)\n",
    "\t# calculate accuracy\n",
    "\treturn accuracy_score(testy, yhat)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# fit all models\n",
    "n_members = 10\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "\t# evaluate model with i members\n",
    "\tensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\t# evaluate the i'th model standalone\n",
    "\ttesty_enc = to_categorical(testy)\n",
    "\t_, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "\t# summarize this step\n",
    "\tprint('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "\tensemble_scores.append(ensemble_score)\n",
    "\tsingle_scores.append(single_score)\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for coefficients in a weighted average ensemble for the blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "\ttrainy_enc = to_categorical(trainy)\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "\t# make predictions\n",
    "\tyhats = [model.predict(testX) for model in members]\n",
    "\tyhats = array(yhats)\n",
    "\t# weighted sum across ensemble members\n",
    "\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "\t# argmax across classes\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\treturn result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "\t# make prediction\n",
    "\tyhat = ensemble_predictions(members, weights, testX)\n",
    "\t# calculate accuracy\n",
    "\treturn accuracy_score(testy, yhat)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "\n",
    "# grid search weights\n",
    "def grid_search(members, testX, testy):\n",
    "\t# define weights to consider\n",
    "\tw = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\tbest_score, best_weights = 0.0, None\n",
    "\t# iterate all possible combinations (cartesian product)\n",
    "\tfor weights in product(w, repeat=len(members)):\n",
    "\t\t# skip if all weights are equal\n",
    "\t\tif len(set(weights)) == 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# hack, normalize weight vector\n",
    "\t\tweights = normalize(weights)\n",
    "\t\t# evaluate weights\n",
    "\t\tscore = evaluate_ensemble(members, weights, testX, testy)\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score, best_weights = score, weights\n",
    "\t\t\tprint('>%s %.3f' % (best_weights, best_score))\n",
    "\treturn list(best_weights)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "# evaluate each single model on the test set\n",
    "testy_enc = to_categorical(testy)\n",
    "for i in range(n_members):\n",
    "\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model %d: %.3f' % (i+1, test_acc))\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Equal Weights Score: %.3f' % score)\n",
    "# grid search weights\n",
    "weights = grid_search(members, testX, testy)\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Grid Search Weights: %s, Score: %.3f' % (weights, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global optimization to find coefficients for weighted ensemble on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import tensordot\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "\ttrainy_enc = to_categorical(trainy)\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=2, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "\t# make predictions\n",
    "\tyhats = [model.predict(testX) for model in members]\n",
    "\tyhats = array(yhats)\n",
    "\t# weighted sum across ensemble members\n",
    "\tsummed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "\t# argmax across classes\n",
    "\tresult = argmax(summed, axis=1)\n",
    "\treturn result\n",
    "\n",
    "# # evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "\t# make prediction\n",
    "\tyhat = ensemble_predictions(members, weights, testX)\n",
    "\t# calculate accuracy\n",
    "\treturn accuracy_score(testy, yhat)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "\t# calculate l1 vector norm\n",
    "\tresult = norm(weights, 1)\n",
    "\t# check for a vector of all zeros\n",
    "\tif result == 0.0:\n",
    "\t\treturn weights\n",
    "\t# return normalized vector (unit norm)\n",
    "\treturn weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "\t# normalize weights\n",
    "\tnormalized = normalize(weights)\n",
    "\t# calculate error rate\n",
    "\treturn 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "# evaluate each single model on the test set\n",
    "testy_enc = to_categorical(testy)\n",
    "for i in range(n_members):\n",
    "\t_, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
    "\tprint('Model %d: %.3f' % (i+1, test_acc))\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Equal Weights Score: %.3f' % score)\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0)  for _ in range(n_members)]\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=1000, tol=1e-7)\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Optimized Weights Score: %.3f' % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
