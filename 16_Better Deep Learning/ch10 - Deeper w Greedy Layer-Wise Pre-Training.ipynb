{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "\t# select indices of points with the class label\n",
    "\trow_ix = where(y == class_value)\n",
    "\t# scatter plot for points with a different color\n",
    "\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised greedy layer-wise pretraining for blobs classification problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data():\n",
    "\t# generate 2d classification dataset\n",
    "\tX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\t# one hot encode output variable\n",
    "\ty = to_categorical(y)\n",
    "\t# split into train and test\n",
    "\tn_train = 500\n",
    "\ttrainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "\ttrainy, testy = y[:n_train], y[n_train:]\n",
    "\treturn trainX, testX, trainy, testy\n",
    "\n",
    "# define and fit the base model\n",
    "def get_base_model(trainX, trainy):\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(10, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy, epochs=100, verbose=0)\n",
    "\treturn model\n",
    "\n",
    "# evaluate a fit model\n",
    "def evaluate_model(model, trainX, testX, trainy, testy):\n",
    "\t_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\treturn train_acc, test_acc\n",
    "\n",
    "# add one new layer and re-train only the new layer\n",
    "def add_layer(model, trainX, trainy):\n",
    "\t# remember the current output layer\n",
    "\toutput_layer = model.layers[-1]\n",
    "\t# remove the output layer\n",
    "\tmodel.pop()\n",
    "\t# mark all remaining layers as non-trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add a new hidden layer\n",
    "\tmodel.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n",
    "\t# re-add the output layer\n",
    "\tmodel.add(output_layer)\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy, epochs=100, verbose=0)\n",
    "\n",
    "# prepare data\n",
    "trainX, testX, trainy, testy = prepare_data()\n",
    "# get the base model\n",
    "model = get_base_model(trainX, trainy)\n",
    "# evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "# add layers and evaluate the updated model\n",
    "n_layers = 10\n",
    "for i in range(n_layers):\n",
    "\t# add layer\n",
    "\tadd_layer(model, trainX, trainy)\n",
    "\t# evaluate model\n",
    "\ttrain_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n",
    "\tprint('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "\t# store scores for plotting\n",
    "\tscores[len(model.layers)] = (train_acc, test_acc)\n",
    "# plot number of added layers vs accuracy\n",
    "pyplot.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.')\n",
    "pyplot.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised greedy layer-wise pretraining for blobs classification problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data():\n",
    "\t# generate 2d classification dataset\n",
    "\tX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\t# one hot encode output variable\n",
    "\ty = to_categorical(y)\n",
    "\t# split into train and test\n",
    "\tn_train = 500\n",
    "\ttrainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "\ttrainy, testy = y[:n_train], y[n_train:]\n",
    "\treturn trainX, testX, trainy, testy\n",
    "\n",
    "# define, fit and evaluate the base autoencoder\n",
    "def base_autoencoder(trainX, testX):\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(10, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(2, activation='linear'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='mse', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainX, epochs=100, verbose=0)\n",
    "\t# evaluate reconstruction loss\n",
    "\ttrain_mse = model.evaluate(trainX, trainX, verbose=0)\n",
    "\ttest_mse = model.evaluate(testX, testX, verbose=0)\n",
    "\tprint('> reconstruction error train=%.3f, test=%.3f' % (train_mse, test_mse))\n",
    "\treturn model\n",
    "\n",
    "# evaluate the autoencoder as a classifier\n",
    "def evaluate_autoencoder_as_classifier(model, trainX, trainy, testX, testy):\n",
    "\t# remember the current output layer\n",
    "\toutput_layer = model.layers[-1]\n",
    "\t# remove the output layer\n",
    "\tmodel.pop()\n",
    "\t# mark all remaining layers as non-trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add new output layer\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainy, epochs=100, verbose=0)\n",
    "\t# evaluate model\n",
    "\t_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\t# put the model back together\n",
    "\tmodel.pop()\n",
    "\tmodel.add(output_layer)\n",
    "\tmodel.compile(loss='mse', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "\treturn train_acc, test_acc\n",
    "\n",
    "# add one new layer and re-train only the new layer\n",
    "def add_layer_to_autoencoder(model, trainX, testX):\n",
    "\t# remember the current output layer\n",
    "\toutput_layer = model.layers[-1]\n",
    "\t# remove the output layer\n",
    "\tmodel.pop()\n",
    "\t# mark all remaining layers as non-trainable\n",
    "\tfor layer in model.layers:\n",
    "\t\tlayer.trainable = False\n",
    "\t# add a new hidden layer\n",
    "\tmodel.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n",
    "\t# re-add the output layer\n",
    "\tmodel.add(output_layer)\n",
    "\t# fit model\n",
    "\tmodel.fit(trainX, trainX, epochs=100, verbose=0)\n",
    "\t# evaluate reconstruction loss\n",
    "\ttrain_mse = model.evaluate(trainX, trainX, verbose=0)\n",
    "\ttest_mse = model.evaluate(testX, testX, verbose=0)\n",
    "\tprint('> reconstruction error train=%.3f, test=%.3f' % (train_mse, test_mse))\n",
    "\n",
    "# prepare data\n",
    "trainX, testX, trainy, testy = prepare_data()\n",
    "# get the base autoencoder\n",
    "model = base_autoencoder(trainX, testX)\n",
    "# evaluate the base model\n",
    "scores = dict()\n",
    "train_acc, test_acc = evaluate_autoencoder_as_classifier(model, trainX, trainy, testX, testy)\n",
    "print('> classifier accuracy layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "scores[len(model.layers)] = (train_acc, test_acc)\n",
    "# add layers and evaluate the updated model\n",
    "n_layers = 5\n",
    "for _ in range(n_layers):\n",
    "\t# add layer\n",
    "\tadd_layer_to_autoencoder(model, trainX, testX)\n",
    "\t# evaluate model\n",
    "\ttrain_acc, test_acc = evaluate_autoencoder_as_classifier(model, trainX, trainy, testX, testy)\n",
    "\tprint('> classifier accuracy layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "\t# store scores for plotting\n",
    "\tscores[len(model.layers)] = (train_acc, test_acc)\n",
    "# plot number of added layers vs accuracy\n",
    "keys = list(scores.keys())\n",
    "pyplot.plot(keys, [scores[k][0] for k in keys], label='train', marker='.')\n",
    "pyplot.plot(keys, [scores[k][1] for k in keys], label='test', marker='.')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
