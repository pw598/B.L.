{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff9ebb2-96ab-4cbc-ad3a-435b9963b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ae795a-a47d-4fba-ac1c-6ea9cf614452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0: accuracy = 61.12%\n",
      "End of epoch 1: accuracy = 63.96%\n",
      "End of epoch 2: accuracy = 51.43%\n",
      "End of epoch 3: accuracy = 64.94%\n",
      "End of epoch 4: accuracy = 60.35%\n",
      "End of epoch 5: accuracy = 62.81%\n",
      "End of epoch 6: accuracy = 64.95%\n",
      "End of epoch 7: accuracy = 61.87%\n",
      "End of epoch 8: accuracy = 59.55%\n",
      "End of epoch 9: accuracy = 58.54%\n",
      "End of epoch 10: accuracy = 70.79%\n",
      "End of epoch 11: accuracy = 70.73%\n",
      "End of epoch 12: accuracy = 63.75%\n",
      "End of epoch 13: accuracy = 64.97%\n",
      "End of epoch 14: accuracy = 61.10%\n",
      "End of epoch 15: accuracy = 59.97%\n",
      "End of epoch 16: accuracy = 65.61%\n",
      "End of epoch 17: accuracy = 58.04%\n",
      "End of epoch 18: accuracy = 75.76%\n",
      "End of epoch 19: accuracy = 58.36%\n",
      "End of epoch 20: accuracy = 65.27%\n",
      "End of epoch 21: accuracy = 61.19%\n",
      "End of epoch 22: accuracy = 75.05%\n",
      "End of epoch 23: accuracy = 61.39%\n",
      "End of epoch 24: accuracy = 72.43%\n",
      "End of epoch 25: accuracy = 65.93%\n",
      "End of epoch 26: accuracy = 66.84%\n",
      "End of epoch 27: accuracy = 70.33%\n",
      "End of epoch 28: accuracy = 64.23%\n",
      "End of epoch 29: accuracy = 60.35%\n",
      "End of epoch 30: accuracy = 73.31%\n",
      "End of epoch 31: accuracy = 59.38%\n",
      "End of epoch 32: accuracy = 69.86%\n",
      "End of epoch 33: accuracy = 74.05%\n",
      "End of epoch 34: accuracy = 67.93%\n",
      "End of epoch 35: accuracy = 61.37%\n",
      "End of epoch 36: accuracy = 66.98%\n",
      "End of epoch 37: accuracy = 54.41%\n",
      "End of epoch 38: accuracy = 63.41%\n",
      "End of epoch 39: accuracy = 62.89%\n",
      "End of epoch 40: accuracy = 77.11%\n",
      "End of epoch 41: accuracy = 75.21%\n",
      "End of epoch 42: accuracy = 72.82%\n",
      "End of epoch 43: accuracy = 74.93%\n",
      "End of epoch 44: accuracy = 69.46%\n",
      "End of epoch 45: accuracy = 77.76%\n",
      "End of epoch 46: accuracy = 68.83%\n",
      "End of epoch 47: accuracy = 59.85%\n",
      "End of epoch 48: accuracy = 76.47%\n",
      "End of epoch 49: accuracy = 70.97%\n",
      "End of epoch 50: accuracy = 70.07%\n",
      "End of epoch 51: accuracy = 76.39%\n",
      "End of epoch 52: accuracy = 71.60%\n",
      "End of epoch 53: accuracy = 52.79%\n",
      "End of epoch 54: accuracy = 76.27%\n",
      "End of epoch 55: accuracy = 76.46%\n",
      "End of epoch 56: accuracy = 71.24%\n",
      "End of epoch 57: accuracy = 69.18%\n",
      "End of epoch 58: accuracy = 71.49%\n",
      "End of epoch 59: accuracy = 72.66%\n",
      "End of epoch 60: accuracy = 77.55%\n",
      "End of epoch 61: accuracy = 57.91%\n",
      "End of epoch 62: accuracy = 69.54%\n",
      "End of epoch 63: accuracy = 64.33%\n",
      "End of epoch 64: accuracy = 72.73%\n",
      "End of epoch 65: accuracy = 77.02%\n",
      "End of epoch 66: accuracy = 77.75%\n",
      "End of epoch 67: accuracy = 72.27%\n",
      "End of epoch 68: accuracy = 76.55%\n",
      "End of epoch 69: accuracy = 77.81%\n",
      "End of epoch 70: accuracy = 75.31%\n",
      "End of epoch 71: accuracy = 70.87%\n",
      "End of epoch 72: accuracy = 77.36%\n",
      "End of epoch 73: accuracy = 77.94%\n",
      "End of epoch 74: accuracy = 77.66%\n",
      "End of epoch 75: accuracy = 68.10%\n",
      "End of epoch 76: accuracy = 64.74%\n",
      "End of epoch 77: accuracy = 76.50%\n",
      "End of epoch 78: accuracy = 77.90%\n",
      "End of epoch 79: accuracy = 77.53%\n",
      "End of epoch 80: accuracy = 66.84%\n",
      "End of epoch 81: accuracy = 77.14%\n",
      "End of epoch 82: accuracy = 52.11%\n",
      "End of epoch 83: accuracy = 77.52%\n",
      "End of epoch 84: accuracy = 63.85%\n",
      "End of epoch 85: accuracy = 74.78%\n",
      "End of epoch 86: accuracy = 67.56%\n",
      "End of epoch 87: accuracy = 65.54%\n",
      "End of epoch 88: accuracy = 72.66%\n",
      "End of epoch 89: accuracy = 74.37%\n",
      "End of epoch 90: accuracy = 66.68%\n",
      "End of epoch 91: accuracy = 76.57%\n",
      "End of epoch 92: accuracy = 69.46%\n",
      "End of epoch 93: accuracy = 65.88%\n",
      "End of epoch 94: accuracy = 73.43%\n",
      "End of epoch 95: accuracy = 72.71%\n",
      "End of epoch 96: accuracy = 66.93%\n",
      "End of epoch 97: accuracy = 74.85%\n",
      "End of epoch 98: accuracy = 70.36%\n",
      "End of epoch 99: accuracy = 77.02%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, default_collate\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = fetch_openml(\"electricity\", version=1, parser=\"auto\")\n",
    "\n",
    "# Label encode the target, convert to float tensors\n",
    "X = data['data'].astype('float').values\n",
    "y = data['target']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# train-test split for model evaluation\n",
    "trainset, testset = random_split(TensorDataset(X, y), [0.7, 0.3])\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "loader = DataLoader(trainset, shuffle=True, batch_size=32)\n",
    "X_test, y_test = default_collate(testset)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    print(f\"End of epoch {epoch}: accuracy = {float(acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8952515d-ac4f-4605-b406-3d2dd00e2420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0: accuracy = 57.25%\n",
      "End of epoch 1: accuracy = 59.85%\n",
      "End of epoch 2: accuracy = 64.24%\n",
      "End of epoch 3: accuracy = 67.09%\n",
      "End of epoch 4: accuracy = 64.86%\n",
      "End of epoch 5: accuracy = 65.64%\n",
      "End of epoch 6: accuracy = 67.00%\n",
      "End of epoch 7: accuracy = 65.49%\n",
      "End of epoch 8: accuracy = 59.60%\n",
      "End of epoch 9: accuracy = 71.98%\n",
      "End of epoch 10: accuracy = 65.28%\n",
      "End of epoch 11: accuracy = 72.66%\n",
      "End of epoch 12: accuracy = 69.79%\n",
      "End of epoch 13: accuracy = 73.87%\n",
      "End of epoch 14: accuracy = 61.04%\n",
      "End of epoch 15: accuracy = 71.06%\n",
      "End of epoch 16: accuracy = 66.09%\n",
      "End of epoch 17: accuracy = 63.33%\n",
      "End of epoch 18: accuracy = 68.34%\n",
      "End of epoch 19: accuracy = 62.83%\n",
      "End of epoch 20: accuracy = 59.90%\n",
      "End of epoch 21: accuracy = 56.83%\n",
      "End of epoch 22: accuracy = 63.78%\n",
      "End of epoch 23: accuracy = 75.03%\n",
      "End of epoch 24: accuracy = 66.64%\n",
      "End of epoch 25: accuracy = 69.62%\n",
      "End of epoch 26: accuracy = 75.97%\n",
      "End of epoch 27: accuracy = 75.65%\n",
      "End of epoch 28: accuracy = 62.11%\n",
      "End of epoch 29: accuracy = 71.16%\n",
      "End of epoch 30: accuracy = 75.47%\n",
      "End of epoch 31: accuracy = 70.40%\n",
      "End of epoch 32: accuracy = 68.62%\n",
      "End of epoch 33: accuracy = 71.63%\n",
      "End of epoch 34: accuracy = 69.84%\n",
      "End of epoch 35: accuracy = 75.99%\n",
      "End of epoch 36: accuracy = 77.23%\n",
      "End of epoch 37: accuracy = 77.61%\n",
      "End of epoch 38: accuracy = 76.09%\n",
      "End of epoch 39: accuracy = 61.65%\n",
      "End of epoch 40: accuracy = 76.05%\n",
      "End of epoch 41: accuracy = 65.25%\n",
      "End of epoch 42: accuracy = 65.40%\n",
      "End of epoch 43: accuracy = 77.48%\n",
      "End of epoch 44: accuracy = 62.16%\n",
      "End of epoch 45: accuracy = 75.67%\n",
      "End of epoch 46: accuracy = 66.50%\n",
      "End of epoch 47: accuracy = 72.98%\n",
      "End of epoch 48: accuracy = 61.63%\n",
      "End of epoch 49: accuracy = 68.62%\n",
      "End of epoch 50: accuracy = 75.56%\n",
      "End of epoch 51: accuracy = 66.69%\n",
      "End of epoch 52: accuracy = 60.22%\n",
      "End of epoch 53: accuracy = 73.96%\n",
      "End of epoch 54: accuracy = 62.68%\n",
      "End of epoch 55: accuracy = 68.34%\n",
      "End of epoch 56: accuracy = 75.94%\n",
      "End of epoch 57: accuracy = 68.05%\n",
      "End of epoch 58: accuracy = 77.36%\n",
      "End of epoch 59: accuracy = 76.72%\n",
      "End of epoch 60: accuracy = 70.62%\n",
      "End of epoch 61: accuracy = 72.07%\n",
      "End of epoch 62: accuracy = 77.00%\n",
      "End of epoch 63: accuracy = 58.24%\n",
      "End of epoch 64: accuracy = 68.49%\n",
      "End of epoch 65: accuracy = 68.39%\n",
      "End of epoch 66: accuracy = 76.63%\n",
      "End of epoch 67: accuracy = 70.89%\n",
      "End of epoch 68: accuracy = 72.25%\n",
      "End of epoch 69: accuracy = 77.66%\n",
      "End of epoch 70: accuracy = 74.83%\n",
      "End of epoch 71: accuracy = 76.47%\n",
      "End of epoch 72: accuracy = 64.17%\n",
      "End of epoch 73: accuracy = 73.49%\n",
      "End of epoch 74: accuracy = 73.66%\n",
      "End of epoch 75: accuracy = 75.27%\n",
      "End of epoch 76: accuracy = 66.98%\n",
      "End of epoch 77: accuracy = 77.93%\n",
      "End of epoch 78: accuracy = 72.82%\n",
      "End of epoch 79: accuracy = 71.90%\n",
      "End of epoch 80: accuracy = 67.65%\n",
      "End of epoch 81: accuracy = 69.71%\n",
      "End of epoch 82: accuracy = 64.11%\n",
      "End of epoch 83: accuracy = 73.79%\n",
      "End of epoch 84: accuracy = 73.84%\n",
      "End of epoch 85: accuracy = 74.07%\n",
      "End of epoch 86: accuracy = 60.17%\n",
      "End of epoch 87: accuracy = 73.28%\n",
      "End of epoch 88: accuracy = 76.54%\n",
      "End of epoch 89: accuracy = 71.60%\n",
      "End of epoch 90: accuracy = 74.66%\n",
      "End of epoch 91: accuracy = 75.62%\n",
      "End of epoch 92: accuracy = 76.08%\n",
      "End of epoch 93: accuracy = 77.37%\n",
      "End of epoch 94: accuracy = 71.85%\n",
      "End of epoch 95: accuracy = 73.98%\n",
      "End of epoch 96: accuracy = 75.72%\n",
      "End of epoch 97: accuracy = 76.41%\n",
      "End of epoch 98: accuracy = 77.12%\n",
      "End of epoch 99: accuracy = 78.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, default_collate\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "\n",
    "data = fetch_openml(\"electricity\", version=1, parser=\"auto\")\n",
    "\n",
    "# Label encode the target, convert to float tensors\n",
    "X = data['data'].astype('float').values\n",
    "y = data['target']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# train-test split for model evaluation\n",
    "trainset, testset = random_split(TensorDataset(X, y), [0.7, 0.3])\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "loader = DataLoader(trainset, shuffle=True, batch_size=32)\n",
    "X_test, y_test = default_collate(testset)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "start_epoch = 0\n",
    "if start_epoch > 0:\n",
    "    resume_epoch = start_epoch - 1\n",
    "    resume(model, f\"epoch-{resume_epoch}.pth\")\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    print(f\"End of epoch {epoch}: accuracy = {float(acc)*100:.2f}%\")\n",
    "    checkpoint(model, f\"epoch-{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8774c1d5-9902-4a98-92ae-266d178c2ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0: accuracy = 60.10%\n",
      "End of epoch 1: accuracy = 61.58%\n",
      "End of epoch 2: accuracy = 62.69%\n",
      "End of epoch 3: accuracy = 63.60%\n",
      "End of epoch 4: accuracy = 65.08%\n",
      "End of epoch 5: accuracy = 63.26%\n",
      "End of epoch 6: accuracy = 62.67%\n",
      "End of epoch 7: accuracy = 66.36%\n",
      "End of epoch 8: accuracy = 65.00%\n",
      "End of epoch 9: accuracy = 67.17%\n",
      "End of epoch 10: accuracy = 63.78%\n",
      "End of epoch 11: accuracy = 63.48%\n",
      "End of epoch 12: accuracy = 65.58%\n",
      "End of epoch 13: accuracy = 62.30%\n",
      "End of epoch 14: accuracy = 69.59%\n",
      "End of epoch 15: accuracy = 69.41%\n",
      "End of epoch 16: accuracy = 71.25%\n",
      "End of epoch 17: accuracy = 53.79%\n",
      "End of epoch 18: accuracy = 70.32%\n",
      "End of epoch 19: accuracy = 64.51%\n",
      "End of epoch 20: accuracy = 71.07%\n",
      "End of epoch 21: accuracy = 65.06%\n",
      "End of epoch 22: accuracy = 72.87%\n",
      "End of epoch 23: accuracy = 66.60%\n",
      "End of epoch 24: accuracy = 70.28%\n",
      "End of epoch 25: accuracy = 69.58%\n",
      "End of epoch 26: accuracy = 71.99%\n",
      "End of epoch 27: accuracy = 63.39%\n",
      "End of epoch 28: accuracy = 73.54%\n",
      "End of epoch 29: accuracy = 64.67%\n",
      "End of epoch 30: accuracy = 74.19%\n",
      "End of epoch 31: accuracy = 58.03%\n",
      "End of epoch 32: accuracy = 65.34%\n",
      "End of epoch 33: accuracy = 62.08%\n",
      "End of epoch 34: accuracy = 73.71%\n",
      "End of epoch 35: accuracy = 70.35%\n",
      "End of epoch 36: accuracy = 66.42%\n",
      "End of epoch 37: accuracy = 73.58%\n",
      "End of epoch 38: accuracy = 74.24%\n",
      "End of epoch 39: accuracy = 69.96%\n",
      "End of epoch 40: accuracy = 66.31%\n",
      "End of epoch 41: accuracy = 69.39%\n",
      "End of epoch 42: accuracy = 73.85%\n",
      "End of epoch 43: accuracy = 75.69%\n",
      "End of epoch 44: accuracy = 73.63%\n",
      "End of epoch 45: accuracy = 76.00%\n",
      "End of epoch 46: accuracy = 74.47%\n",
      "End of epoch 47: accuracy = 68.36%\n",
      "End of epoch 48: accuracy = 68.34%\n",
      "End of epoch 49: accuracy = 53.06%\n",
      "End of epoch 50: accuracy = 65.23%\n",
      "End of epoch 51: accuracy = 74.63%\n",
      "End of epoch 52: accuracy = 59.41%\n",
      "End of epoch 53: accuracy = 69.31%\n",
      "End of epoch 54: accuracy = 66.20%\n",
      "End of epoch 55: accuracy = 71.64%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m---> 58\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[0;32m     60\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, default_collate\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = fetch_openml(\"electricity\", version=1, parser=\"auto\")\n",
    "\n",
    "# Label encode the target, convert to float tensors\n",
    "X = data['data'].astype('float').values\n",
    "y = data['target']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# train-test split for model evaluation\n",
    "trainset, testset = random_split(TensorDataset(X, y), [0.7, 0.3])\n",
    "\n",
    "def checkpoint(model, optimizer, filename):\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, filename)\n",
    "\n",
    "def resume(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "start_epoch = 0\n",
    "loader = DataLoader(trainset, shuffle=True, batch_size=32)\n",
    "X_test, y_test = default_collate(testset)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "if start_epoch > 0:\n",
    "    resume_epoch = start_epoch - 1\n",
    "    resume(model, optimizer, f\"epoch-{resume_epoch}.pth\")\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    print(f\"End of epoch {epoch}: accuracy = {float(acc)*100:.2f}%\")\n",
    "    checkpoint(model, optimizer, f\"epoch-{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67812be-5da3-480d-a6f6-e7b61a6be3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0: accuracy = 61.27%\n",
      "End of epoch 1: accuracy = 62.75%\n",
      "End of epoch 2: accuracy = 62.97%\n",
      "End of epoch 3: accuracy = 63.82%\n",
      "End of epoch 4: accuracy = 61.83%\n",
      "End of epoch 5: accuracy = 64.95%\n",
      "End of epoch 6: accuracy = 62.89%\n",
      "End of epoch 7: accuracy = 63.68%\n",
      "End of epoch 8: accuracy = 69.08%\n",
      "End of epoch 9: accuracy = 64.16%\n",
      "End of epoch 10: accuracy = 61.70%\n",
      "End of epoch 11: accuracy = 63.86%\n",
      "End of epoch 12: accuracy = 61.62%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     50\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     52\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     53\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, default_collate\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = fetch_openml(\"electricity\", version=1, parser=\"auto\")\n",
    "\n",
    "# Label encode the target, convert to float tensors\n",
    "X = data['data'].astype('float').values\n",
    "y = data['target']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# train-test split for model evaluation\n",
    "trainset, testset = random_split(TensorDataset(X, y), [0.7, 0.3])\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 10000  # more than we needed\n",
    "loader = DataLoader(trainset, shuffle=True, batch_size=32)\n",
    "X_test, y_test = default_collate(testset)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "early_stop_thresh = 5\n",
    "best_accuracy = -1\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    acc = float(acc) * 100\n",
    "    print(f\"End of epoch {epoch}: accuracy = {acc:.2f}%\")\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_epoch = epoch\n",
    "        checkpoint(model, \"best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "\n",
    "resume(model, \"best_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
