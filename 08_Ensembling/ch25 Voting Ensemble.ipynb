{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare hard voting to standalone classifiers\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of standalone models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# define the number of neighbors to consider\n",
    "\tneighbors = [1, 3, 5, 7, 9]\n",
    "\tfor n in neighbors:\n",
    "\t\tkey = 'knn' + str(n)\n",
    "\t\tmodels[key] = KNeighborsClassifier(n_neighbors=n)\n",
    "\t# define the voting ensemble\n",
    "\tmembers = [(n,m) for n,m in models.items()]\n",
    "\tmodels['hard_voting'] = VotingClassifier(estimators=members, voting='hard')\n",
    "\treturn models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize the performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a hard voting ensemble\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "# define the base models\n",
    "models = list()\n",
    "neighbors = [1, 3, 5, 7, 9]\n",
    "for n in neighbors:\n",
    "\tmodels.append(('knn'+str(n), KNeighborsClassifier(n_neighbors=n)))\n",
    "# define the hard voting ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "# fit the model on all available data\n",
    "ensemble.fit(X, y)\n",
    "# make a prediction for one example\n",
    "row = [5.88891819, 2.64867662, -0.42728226, -1.24988856, -0.00822, -3.57895574, 2.87938412, -1.55614691, -0.38168784, 7.50285659, -1.16710354, -5.02492712, -0.46196105, -0.64539455, -1.71297469, 0.25987852, -0.193401, -5.52022952, 0.0364453, -1.960039]\n",
    "# summarize the prediction\n",
    "yhat = ensemble.predict([row])\n",
    "print('Predicted Class: %d' % (yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare soft voting ensemble to standalone classifiers\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of standalone models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# define the degrees to consider\n",
    "\tfor n in range(1,6):\n",
    "\t\tkey = 'svm' + str(n)\n",
    "\t\tmodels[key] = SVC(probability=True, kernel='poly', degree=n)\n",
    "\t# define the voting ensemble\n",
    "\tmembers = [(n,m) for n,m in models.items()]\n",
    "\tmodels['soft_voting'] = VotingClassifier(estimators=members, voting='soft')\n",
    "\treturn models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize the performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a soft voting ensemble\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "# define the base models\n",
    "models = list()\n",
    "for n in range(1,6):\n",
    "\tmodels.append(('svm'+str(n), SVC(probability=True, kernel='poly', degree=n)))\n",
    "# define the soft voting ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "# fit the model on all available data\n",
    "ensemble.fit(X, y)\n",
    "# make a prediction for one example\n",
    "row = [5.88891819, 2.64867662, -0.42728226, -1.24988856, -0.00822, -3.57895574, 2.87938412, -1.55614691, -0.38168784, 7.50285659, -1.16710354, -5.02492712, -0.46196105, -0.64539455, -1.71297469, 0.25987852, -0.193401, -5.52022952, 0.0364453, -1.960039]\n",
    "yhat = ensemble.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % (yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare voting ensemble to each standalone models for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of standalone models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# define the tree depths to consider\n",
    "\tfor n in range(1,6):\n",
    "\t\tkey = 'cart' + str(n)\n",
    "\t\tmodels[key] = DecisionTreeRegressor(max_depth=n)\n",
    "\t# define the voting ensemble\n",
    "\tmembers = [(n,m) for n,m in models.items()]\n",
    "\tmodels['voting'] = VotingRegressor(estimators=members)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize the performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a voting ensemble\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=1)\n",
    "# define the base models\n",
    "models = list()\n",
    "for n in range(1,6):\n",
    "\tmodels.append(('cart'+str(n), DecisionTreeRegressor(max_depth=n)))\n",
    "# define the voting ensemble\n",
    "ensemble = VotingRegressor(estimators=models)\n",
    "# fit the model on all available data\n",
    "ensemble.fit(X, y)\n",
    "# make a prediction for one example\n",
    "row = [0.59332206, -0.56637507, 1.34808718, -0.57054047, -0.72480487, 1.05648449, 0.77744852, 0.07361796, 0.88398267, 2.02843157, 1.01902732, 0.11227799, 0.94218853, 0.26741783, 0.91458143, -0.72759572, 1.08842814, -0.61450942, -0.69387293, 1.69169009]\n",
    "yhat = ensemble.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted Value: %.3f' % (yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
