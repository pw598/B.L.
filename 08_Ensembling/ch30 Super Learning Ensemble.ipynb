{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a super learner model for binary classification\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# collect out of fold predictions from cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict_proba(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    "\n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    "\n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LogisticRegression(solver='liblinear')\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tscore = accuracy_score(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, score))\n",
    "\n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict_proba(X)\n",
    "\t\tmeta_X.append(yhat)\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "fit_base_models(X, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate base models\n",
    "evaluate_models(X_val, y_val, models)\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "score = accuracy_score(y_val, yhat)\n",
    "print('Super Learner: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a super learner model for regression\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# collect out of fold predictions from cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t# define split of data\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t# store columns\n",
    "\t\t\tfold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    "\n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    "\n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LinearRegression()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmae = mean_absolute_error(y, yhat)\n",
    "\t\tprint('%s: MAE %.3f' % (model.__class__.__name__, mae))\n",
    "\n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5, random_state=1)\n",
    "# split\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "fit_base_models(X, y, models)\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate base models\n",
    "evaluate_models(X_val, y_val, models)\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "score = mean_absolute_error(y_val, yhat)\n",
    "print('Super Learner: MAE %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the version of the mlens library\n",
    "import mlens\n",
    "# report library version\n",
    "print(mlens.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluating a super learner ensemble for classification with the mlens library\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "# create the super learner\n",
    "ensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X_train))\n",
    "# add the base models\n",
    "ensemble.add(get_models())\n",
    "# add the meta model\n",
    "ensemble.add_meta(LogisticRegression(solver='lbfgs'))\n",
    "# fit the super learner\n",
    "ensemble.fit(X_train, y_train)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# make predictions on hold out set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Super Learner Accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluating a super learner ensemble for regression with the mlens library\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# create a list of base-models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(ElasticNet())\n",
    "\tmodels.append(SVR(gamma='scale'))\n",
    "\tmodels.append(DecisionTreeRegressor())\n",
    "\tmodels.append(KNeighborsRegressor())\n",
    "\tmodels.append(AdaBoostRegressor())\n",
    "\tmodels.append(BaggingRegressor(n_estimators=10))\n",
    "\tmodels.append(RandomForestRegressor(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesRegressor(n_estimators=10))\n",
    "\treturn models\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5, random_state=1)\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "# create the super learner\n",
    "ensemble = SuperLearner(scorer=mean_absolute_error, folds=10, shuffle=True, sample_size=len(X_train))\n",
    "# add the base models\n",
    "ensemble.add(get_models())\n",
    "# add the meta model\n",
    "ensemble.add_meta(LinearRegression())\n",
    "# fit the super learner\n",
    "ensemble.fit(X_train, y_train)\n",
    "# summarize base learners\n",
    "print(ensemble.data)\n",
    "# make predictions on hold out set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print('Super Learner MAE: %.3f' % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
