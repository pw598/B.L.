{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic binary classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a weighted average ensemble for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(('lr', LogisticRegression()))\n",
    "\tmodels.append(('cart', DecisionTreeClassifier()))\n",
    "\tmodels.append(('bayes', GaussianNB()))\n",
    "\treturn models\n",
    "\n",
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "\t# fit and evaluate the models\n",
    "\tscores = list()\n",
    "\tfor _, model in models:\n",
    "\t\t# fit the model\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\t# evaluate the model\n",
    "\t\tyhat = model.predict(X_val)\n",
    "\t\tacc = accuracy_score(y_val, yhat)\n",
    "\t\t# store the performance\n",
    "\t\tscores.append(acc)\n",
    "\t\t# report model performance\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# fit and evaluate each model\n",
    "scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "print(scores)\n",
    "# create the ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft', weights=scores)\n",
    "# fit the ensemble on the training dataset\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Weighted Avg Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a weighted average ensemble for classification compared to base model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(('lr', LogisticRegression()))\n",
    "\tmodels.append(('cart', DecisionTreeClassifier()))\n",
    "\tmodels.append(('bayes', GaussianNB()))\n",
    "\treturn models\n",
    "\n",
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "\t# fit and evaluate the models\n",
    "\tscores = list()\n",
    "\tfor _, model in models:\n",
    "\t\t# fit the model\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\t# evaluate the model\n",
    "\t\tyhat = model.predict(X_val)\n",
    "\t\tacc = accuracy_score(y_val, yhat)\n",
    "\t\t# store the performance\n",
    "\t\tscores.append(acc)\n",
    "\t\t# report model performance\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# fit and evaluate each model\n",
    "scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "print(scores)\n",
    "# create the ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft', weights=scores)\n",
    "# fit the ensemble on the training dataset\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Weighted Avg Accuracy: %.3f' % (score*100))\n",
    "# evaluate each standalone model\n",
    "scores = evaluate_models(models, X_train_full, X_test, y_train_full, y_test)\n",
    "for i in range(len(models)):\n",
    "\tprint('>%s: %.3f' % (models[i][0], scores[i]*100))\n",
    "# evaluate equal weighting\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "yhat = ensemble.predict(X_test)\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Voting Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, noise=0.3, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a weighted average ensemble for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(('knn', KNeighborsRegressor()))\n",
    "\tmodels.append(('cart', DecisionTreeRegressor()))\n",
    "\tmodels.append(('svm', SVR()))\n",
    "\treturn models\n",
    "\n",
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "\t# fit and evaluate the models\n",
    "\tscores = list()\n",
    "\tfor _, model in models:\n",
    "\t\t# fit the model\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\t# evaluate the model\n",
    "\t\tyhat = model.predict(X_val)\n",
    "\t\tmae = mean_absolute_error(y_val, yhat)\n",
    "\t\t# store the performance\n",
    "\t\tscores.append(-mae)\n",
    "\t\t# report model performance\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, noise=0.3, random_state=7)\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# fit and evaluate each model\n",
    "scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "print(scores)\n",
    "# create the ensemble\n",
    "ensemble = VotingRegressor(estimators=models, weights=scores)\n",
    "# fit the ensemble on the training dataset\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print('Weighted Avg MAE: %.3f' % (score))\n",
    "# evaluate each standalone model\n",
    "scores = evaluate_models(models, X_train_full, X_test, y_train_full, y_test)\n",
    "for i in range(len(models)):\n",
    "\tprint('>%s: %.3f' % (models[i][0], scores[i]))\n",
    "# evaluate equal weighting\n",
    "ensemble = VotingRegressor(estimators=models)\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "yhat = ensemble.predict(X_test)\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print('Voting MAE: %.3f' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate argsort\n",
    "from numpy import argsort\n",
    "# data\n",
    "x = [300, 100, 200]\n",
    "print(x)\n",
    "# argsort of data\n",
    "print(argsort(x))\n",
    "# arg sort of argsort of data\n",
    "print(argsort(argsort(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate argsort with negative scores\n",
    "from numpy import argsort\n",
    "# data\n",
    "x = [-10, -100, -80]\n",
    "print(x)\n",
    "# argsort of data\n",
    "print(argsort(x))\n",
    "# arg sort of argsort of data\n",
    "print(argsort(argsort(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a weighted average ensemble for regression with rankings for model weights\n",
    "from numpy import argsort\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(('knn', KNeighborsRegressor()))\n",
    "\tmodels.append(('cart', DecisionTreeRegressor()))\n",
    "\tmodels.append(('svm', SVR()))\n",
    "\treturn models\n",
    "\n",
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "\t# fit and evaluate the models\n",
    "\tscores = list()\n",
    "\tfor _, model in models:\n",
    "\t\t# fit the model\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\t\t# evaluate the model\n",
    "\t\tyhat = model.predict(X_val)\n",
    "\t\tmae = mean_absolute_error(y_val, yhat)\n",
    "\t\t# store the performance\n",
    "\t\tscores.append(-mae)\n",
    "\t\t# report model performance\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, noise=0.3, random_state=7)\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# fit and evaluate each model\n",
    "scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "print(scores)\n",
    "ranking = 1 + argsort(argsort(scores))\n",
    "print(ranking)\n",
    "# create the ensemble\n",
    "ensemble = VotingRegressor(estimators=models, weights=ranking)\n",
    "# fit the ensemble on the training dataset\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_test)\n",
    "# evaluate predictions\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print('Weighted Avg MAE: %.3f' % (score))\n",
    "# evaluate each standalone model\n",
    "scores = evaluate_models(models, X_train_full, X_test, y_train_full, y_test)\n",
    "for i in range(len(models)):\n",
    "\tprint('>%s: %.3f' % (models[i][0], scores[i]))\n",
    "# evaluate equal weighting\n",
    "ensemble = VotingRegressor(estimators=models)\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "yhat = ensemble.predict(X_test)\n",
    "score = mean_absolute_error(y_test, yhat)\n",
    "print('Voting MAE: %.3f' % (score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
